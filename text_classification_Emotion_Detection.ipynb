{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification_Emotion-Detection",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eaNODE/waibo/blob/master/text_classification_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiUWuOVT9tTx",
        "colab_type": "text"
      },
      "source": [
        "# **TEXT CLASSIFICATION - EMOTION DETECTION KERAS CODE**\n",
        "*`getting our hands on`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XITDW-cizeC0",
        "colab_type": "code",
        "outputId": "9a8f18d5-8399-44c4-9aed-c54f36802480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -*- encoding: utf-8 -*-\n",
        "\n",
        "\n",
        "import numpy as np, pandas as pd, os, sys, time, datetime, nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential, load_model, save_model, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten, LSTM, Bidirectional, Embedding, Flatten, Conv1D, MaxPooling1D\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "nltk.download('stopwords')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Preprocessing and formating\n",
        "# ===================================\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 20\n",
        "EMBEDDING_DIM = 50\n",
        "dataFrame = pd.read_csv('/gdrive/My Drive/text_emotion_twitter.csv', encoding='utf-8')\n",
        "# dataFrame is a matrix of 40000 rows X 4 cols\n",
        "# dataFrame.values[[rows], [cols]]\n",
        "x = dataFrame.values[:,3] # accessing third elements of all rows which is content/text => 40000 texts/contents\n",
        "y = dataFrame.values[:,1] # accessing first elements of all rows which is label/emotions => 40000 labels/emotions\n",
        "print(\"[+] data size from csv >>> \", x.shape)\n",
        "print(\"[+] label size from csv >>> \", y.shape)\n",
        "\n",
        "\n",
        "def uprint(*objects, sep=' ', end='\\n', file=sys.stdout):\n",
        "    enc = file.encoding\n",
        "    if enc == 'UTF-8':\n",
        "        print(*objects, sep=sep, end=end, file=file)\n",
        "    else:\n",
        "        f = lambda obj: str(obj).encode(enc, errors='backslashreplace').decode(enc)\n",
        "        print(*map(f, objects), sep=sep, end=end, file=file)\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize = (12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label = 'Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label = 'Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label = 'Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label = 'Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "new_stop_words = set(stop_words)\n",
        "# adding woudlnt type of words into stopwords list\n",
        "for s in stop_words:\n",
        "    new_stop_words.add(s.replace('\\'',''))\n",
        "    pass\n",
        "stop_words = new_stop_words\n",
        "print(\"\\n[+] Excluding stopwords ...\")\n",
        "\n",
        "\n",
        "# removing @ from default base filter, to remove that whole word, which might be considered as user or page name\n",
        "base_filters = '\\n\\t!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~ '\n",
        "word_sequences = []\n",
        "for i in x:\n",
        "    i = str(i)\n",
        "#     uprint(i)\n",
        "    i = i.replace('\\'', '')\n",
        "    newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith(\"@\")]\n",
        "    filtered_sentence = [w for w in newlist if not w in stop_words] \n",
        "    word_sequences.append(filtered_sentence)\n",
        "    pass\n",
        "# print(\"\\n[+] word sequences >>> \", word_sequences) # len is : 40000\n",
        "# [ ['know', 'listenin', 'bad', 'habit', 'earlier', 'started', 'freakin', 'part'], ....... , ['layin', 'n', 'bed', 'headache', 'ughhhh', 'waitin', 'call'] ]\n",
        "\n",
        "\n",
        "\n",
        "# Tokenizing words to word indices\n",
        "tokenizer = Tokenizer()\n",
        "# fitting texts to tokenizer object\n",
        "tokenizer.fit_on_texts(word_sequences)\n",
        "word_indices = tokenizer.texts_to_sequences(word_sequences)\n",
        "word_index = tokenizer.word_index # size is 32855 cause its a dict with key as word and index as value\n",
        "# print(\"\\n[+] word index >>> \", word_index) \n",
        "# word_index : { 'im': 1, 'day': 2, 'good': 3, 'get': 4, 'like': 5, ......  , 'quot': 6, 'http': 7, 'go': 8, 'today': 9 }\n",
        "print(\"\\n[+] Tokenized to Word indices as \")\n",
        "print(np.array(word_indices).shape) # size is 40000\n",
        "# print(\"\\n[+] word indices >>> \", word_indices)\n",
        "# word_indices : [ [20, 3077, 57, 4396, 714, 489, 1014, 433], ...... , [8964, 177, 74, 319, 3327, 2077, 192], [2555, 3963, 1598, 143] ]\n",
        "\n",
        "\n",
        "\n",
        "# padding word_indices\n",
        "# >>> pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])\n",
        "# array([[0, 1, 2, 3],\n",
        "#        [3, 4, 5, 6],\n",
        "#        [0, 0, 7, 8]], dtype=int32)\n",
        "\n",
        "x_data = pad_sequences(word_indices, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print(\"\\n[+] After padding data size is\")\n",
        "print(x_data.shape) # (40000, 20)\n",
        "print(\"\\n[+] padded x_data >>> \")\n",
        "print(x_data)\n",
        "# [[    0     0     0 ...   489  1014   433]\n",
        "#  [    0     0     0 ...  3327  2077   192]\n",
        "#  [    0     0     0 ...  3963  1598   143]\n",
        "#  ...\n",
        "#  [    0     0     0 ...  1390   165     2]\n",
        "#  [    0     0     0 ...  1135    19   336]\n",
        "#  [    0     0     0 ... 32852 32853 32854]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Building Embedding Layer\n",
        "# ===================================\n",
        "\n",
        "\n",
        "# using pretrained glove vector\n",
        "print(\"\\n[+] Loading Glove Vectors ...\")\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('', '/gdrive/My Drive/glove.6B.50d.txt'),'r',encoding=\"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "#     print(\"[+] coefs >>> \", coefs)\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('\\n[+] Loaded GloVe Vectors Successfully!')\n",
        "\n",
        "\n",
        "# embedding_matrix size is : vocab + 1 X 50\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(\"\\n[+] Embedding Matrix Generated with size >>>> \", embedding_matrix.shape)\n",
        "print(\"\\n[+] embedding matrix is \")\n",
        "print(embedding_matrix)\n",
        "# it has len(word_index) + 1 rows and 50 cols which is (32855, 50)\n",
        "# [[ 0.          0.          0.         ...  0.          0.\n",
        "#    0.        ]\n",
        "#  [-0.067678    0.51832002  1.32599998 ... -0.65103     0.12924001\n",
        "#    0.48723999]\n",
        "#  [ 0.11626     0.53896999 -0.39513999 ... -0.39061999 -0.10885\n",
        "#    0.084513  ]\n",
        "#  ...\n",
        "#  [ 1.18879998  1.46720004 -0.99624002 ...  0.48704001  0.77978998\n",
        "#    0.38242999]\n",
        "#  [ 0.013849   -0.54549998 -0.077683   ...  0.67878002  0.46202999\n",
        "#    0.72376001]\n",
        "#  [ 0.023778   -0.80124003  0.1193     ... -1.06830001 -0.35703\n",
        "#    0.21013001]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This layer acts as lookup table for vectors, given word index. \n",
        "It will return embedded word vector.\n",
        "Embedding layer can only be used as first layer in Keras.\n",
        "Our input layer will of be size : (None,20) ; None means variable number.\n",
        "As we have padded 20 words for each input, in data preparation stage. \n",
        "we have 20 word indices in each row.\n",
        "Output of Embedding layer will be fed to this LSTM layer.\n",
        "\"\"\"\n",
        "embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False)\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# One Hot encoding Labels\n",
        "# ===================================\n",
        "\n",
        "\n",
        "# Example from sklearn API :\n",
        "\n",
        "# >>> le = preprocessing.LabelEncoder()\n",
        "# >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
        "# LabelEncoder()\n",
        "# >>> list(le.classes_)\n",
        "# ['amsterdam', 'paris', 'tokyo']\n",
        "# >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) \n",
        "# array([2, 2, 1]...)\n",
        "# >>> list(le.inverse_transform([2, 2, 1]))\n",
        "# ['tokyo', 'tokyo', 'paris']\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(y) # Fit label encoder and return encoded labels\n",
        "# print(label_encoder.classes_)\n",
        "# print(label_encoder.transform(label_encoder.classes_)) \n",
        "\n",
        "le_name_mapping = dict(zip(label_encoder.transform(label_encoder.classes_),label_encoder.classes_))\n",
        "print(\"\\n[+] Label Encoding Classes as \")\n",
        "print(le_name_mapping)\n",
        "# {0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "to_categorical\n",
        "\n",
        "Converts a class vector (integers) to binary class matrix.\n",
        "\n",
        "Arguments\n",
        "\n",
        "y: class vector to be converted into a matrix (integers from 0 to num_classes).\n",
        "num_classes: total number of classes.\n",
        "dtype: The data type expected by the input, as a string (float32, float64, int32...)\n",
        "\n",
        "Return\n",
        "\n",
        "A binary matrix representation of the input. The classes axis is placed last.\n",
        "\n",
        "Example\n",
        "\n",
        "# Consider an array of 5 labels out of a set of 3 classes {0, 1, 2}:\n",
        "> labels\n",
        "array([0, 2, 1, 2, 0])\n",
        "# `to_categorical` converts this into a matrix with as many\n",
        "# columns as there are classes. The number of rows\n",
        "# stays the same.\n",
        "> to_categorical(labels)\n",
        "array([[ 1.,  0.,  0.],\n",
        "       [ 0.,  0.,  1.],\n",
        "       [ 0.,  1.,  0.],\n",
        "       [ 0.,  0.,  1.],\n",
        "       [ 1.,  0.,  0.]], dtype=float32)\n",
        "       \n",
        "\"\"\"\n",
        "\n",
        "y_data = np_utils.to_categorical(integer_encoded)\n",
        "print(\"\\n[+] One Hot Encoded class shape with size \")\n",
        "print(y_data.shape) # (40000 , 13)\n",
        "print(\"\\n[+] One hot encoded labels matrix\")\n",
        "print(y_data)\n",
        "# [[0. 0. 1. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===================================\n",
        "# Building Model\n",
        "# ===================================\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "We’ll use LSTM layer with 100 units. This layer has 100 RNN Cells, \n",
        "this number is variable and can be adjusted according to our need and complexity of our data.\n",
        "Input given to LSTM will be considered as (batch_size, timesteps, features).\n",
        "\n",
        "-----------------------\n",
        "return_sequences=True :\n",
        "-----------------------\n",
        "Output of RNN layer will include all the outputs from all the units/cells in that layer.\n",
        "(None, 20,50) => LSTM(100, return_sequences=True) => (None, 20, 100)\n",
        "In the next step, we’ll flatten.\n",
        "(None, 20,100) => Flatten => (None,2000)\n",
        "If you wish to connect a Dense layer directly to an Embedding layer, \n",
        "you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.\n",
        "\n",
        "-----------------------------------------\n",
        "Understanding Dense Layer (Last Layers) :\n",
        "-----------------------------------------\n",
        "We connect all the data that we get from previous levels using Dense Layers.. \n",
        "We keep reducing output units to (None, labels_count) by adding multiple Dense Layers.\n",
        "(None, 2000) => Dense(300) => (None,300)\n",
        "Adding another dense layer\n",
        "(None,300) => Dense(labels_count) => (None,13)\n",
        "13 is labels’ count in our problem. i.e Total number of emotions count.\n",
        "Softmax is probability distribution activation function and helps \n",
        "in achieving better results by distributing probability among labels for a given input.\n",
        "After adding this, we get 13 Outputs each lying between 0 and 1 for each input. \n",
        "Each output represents probability of that emotion for given input. One with highest value can be considered as our prediction.\n",
        "\n",
        "--------\n",
        "Conv1D :\n",
        "--------\n",
        "CNNs work the same way whether they have 1, 2, or 3 dimensions. \n",
        "The difference is the structure of the input data and how the filter, \n",
        "also known as a convolution kernel or feature detector, moves across the data.\n",
        "A 1D CNN is very effective for deriving features from a fixed-length segment \n",
        "of the overall dataset, where it is not so important where the feature is located in the segment.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Conv1D(30, 1, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(4))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(y_data.shape[1], activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "print(\"Finished Preprocessing data ...\")\n",
        "print(\"[+] x_data shape >>> \", x_data.shape)\n",
        "print(\"[+] y_data shape >>> \", y_data.shape)\n",
        "\n",
        "\n",
        "\n",
        "# We gotta split our data into two parts, Training Data, Testing Data\n",
        "# We use Training dataset to train our neural network. \n",
        "# Test dataset to provide an unbiased evaluation of a final model fit on the training dataset.\n",
        "# Split arrays or matrices into random train and test subsets\n",
        "print(\"[+] spliting data into training, testing set ...\")\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data)\n",
        "\n",
        "\n",
        "# # x_train\n",
        "# [[    0     0     0 ...     0    93   866]\n",
        "#  [    0     0     0 ...  1498  5908  7727]\n",
        "#  [    0     0     0 ...   565  2943   162]\n",
        "#  ...\n",
        "#  [    0     0     0 ...  1056 10331 24539]\n",
        "#  [    0     0     0 ...   520   995    71]\n",
        "#  [    0     0     0 ...   565   789   129]] \n",
        "# # x_test\n",
        "# [[    0     0     0 ...    49   595  2060]\n",
        "#  [    0     0     0 ...  2583  3626  2401]\n",
        "#  [    0     0     0 ...  3423  2463   398]\n",
        "#  ...\n",
        "#  [    0     0     0 ...   124    11 18737]\n",
        "#  [    0     0     0 ...    69  8389  8389]\n",
        "#  [    0     0     0 ...    15    27  5723]] \n",
        "# # y_train\n",
        "# [[0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 1.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]] \n",
        "# # y_test\n",
        "# [[0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 1. 0. 0.]\n",
        "#  ...\n",
        "#  [0. 0. 0. ... 0. 0. 1.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]\n",
        "#  [0. 0. 0. ... 0. 0. 0.]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "\n",
        "\"\"\"\n",
        "If you want to build a solid model you have to follow that specific protocol of \n",
        "splitting your data into three sets: One for training, \n",
        "one for validation and one for final evalution, which is the test set.\n",
        "The idea is that you train on your training data and tune your \n",
        "model with the results of metrics (accuracy, loss etc) that you get from your validation set.\n",
        "Your model doesn't \"see\" your validation set and isn´t in any way trained on it, \n",
        "but you as the architect and master of the hyperparameters tune the model according to this data. \n",
        "Therefore it indirectly influences your model because it directly influences \n",
        "your design decisions. You nudge your model to work well with the validation \n",
        "data and that can possibly bring in a tilt.\n",
        "Exactly that is the reason you only evaluate your models final score on data \n",
        "that neither your model nor you yourself has used – and that is the third chunk of data, your test set.\n",
        "Only this procedure makes sure you get an unaffected view of your models quality\n",
        "and ability to generalize what is has learned on totally unseen data.\n",
        "\n",
        "===========\n",
        "Overfitting\n",
        "===========\n",
        "Overfitting occurs when a statistical model or machine learning algorithm captures the noise of the data.  \n",
        "Intuitively, overfitting occurs when the model or the algorithm fits the data too well.  \n",
        "Specifically, overfitting occurs if the model or algorithm shows low bias but high variance.  \n",
        "Overfitting is often a result of an excessively complicated model, and it can be prevented by \n",
        "fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.\n",
        "When training a machine learning model, one of the main things that \n",
        "you want to avoid would be overfitting. This is when your model fits \n",
        "the training data well, but it isn't able to generalize and make \n",
        "accurate predictions for data it hasn't seen before.\n",
        "To find out if their model is overfitting, data scientists \n",
        "use a technique called cross-validation, where they split their \n",
        "data into two parts - the training set, and the validation set. \n",
        "The training set is used to train the model, while the validation set \n",
        "is only used to evaluate the model's performance.\n",
        "Metrics on the training set let you see how your model is progressing in \n",
        "terms of it's training, but it's metrics on the validation set that let \n",
        "you get a measure of the quality of your model - how well it's able to \n",
        "make new predictions based on data it hasn't seen before.\n",
        "With this in mind, loss and acc are measures of loss and accuracy on \n",
        "the training set, while val_loss and val_acc are measures of loss and accuracy \n",
        "on the validation set. At the moment the model has an accuracy of ~38% on the \n",
        "training set and ~40% on the validation set. This means that you can expect your \n",
        "model to perform with ~40% accuracy on new data.\n",
        "as the epochs goes from 10 to 100, your acc metric increases, \n",
        "while your val_acc metric decreases. This means that your model is fitting the \n",
        "training set better, but is losing it's ability to predict on new data, \n",
        "indicating that your model is starting to fit on noise and is beginning to overfit.\n",
        "\n",
        "============\n",
        "Underfitting\n",
        "============\n",
        "Underfitting occurs when a statistical model or machine learning algorithm cannot \n",
        "capture the underlying trend of the data.  Intuitively, underfitting occurs when \n",
        "the model or the algorithm does not fit the data well enough. Specifically, \n",
        "underfitting occurs if the model or algorithm shows low variance but high bias.  \n",
        "Underfitting is often a result of an excessively simple model.\n",
        "\n",
        "======\n",
        "Tricks\n",
        "======\n",
        "If your training loss is much lower than validation loss then \n",
        "this means the network might be overfitting. Solutions to this \n",
        "are to decrease your network size, or to increase dropout. \n",
        "For example you could try dropout of 0.5 and so on.\n",
        "If your training/validation loss are about equal then your model \n",
        "is underfitting. Increase the size of your model \n",
        "(either number of layers or the raw number of neurons per layer)\n",
        "a typical reason for validation accuracy being lower than \n",
        "training accuracy was overfitting. when the opposite is true \n",
        "it’s because my model is underfitting the data.\n",
        "\n",
        "\n",
        "*in all epochs we have underfitting \n",
        ".\n",
        ".\n",
        ".\n",
        ".\n",
        "Epoch 100/100\n",
        "29936/29936 [==============================] - 10s 326us/step - loss: 1.8454 - acc: 0.3736 - val_loss: 1.7907 - val_acc: 0.3906\n",
        "\n",
        "so in our training the model comes into underfitting area , cause we don't have enough dataset! \n",
        "\"\"\"\n",
        "\n",
        "\n",
        "x_valid, y_valid = x_train[:batch_size], y_train[:batch_size]\n",
        "x_train2, y_train2 = x_train[batch_size:], y_train[batch_size:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Save the model after every epoch.\n",
        "filepath can contain named formatting options, which will be filled \n",
        "with the values of epoch and keys in logs (passed in on_epoch_end).\n",
        "For example: if filepath is weights.{epoch:02d}-{val_loss:.2f}.hdf5, \n",
        "then the model checkpoints will be saved with the epoch number and the validation loss in the filename.\n",
        "\n",
        "Arguments\n",
        "\n",
        "filepath: string, path to save the model file.\n",
        "monitor: quantity to monitor.\n",
        "verbose: verbosity mode, 0 or 1.\n",
        "save_best_only: if save_best_only=True, the latest best model according \n",
        "    to the quantity monitored will not be overwritten.\n",
        "    save_weights_only: if True, then only the model's weights will be saved \n",
        "    (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).\n",
        "mode: one of {auto, min, max}. If save_best_only=True, the decision to overwrite \n",
        "    the current save file is made based on either the maximization or the minimization \n",
        "    of the monitored quantity. For val_acc, this should be max, for val_loss this \n",
        "    should be min, etc. In auto mode, the direction is automatically inferred \n",
        "    from the name of the monitored quantity.\n",
        "period: Interval (number of epochs) between checkpoints.\n",
        "\"\"\"\n",
        "\n",
        "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"[+] start time to train >>> {st}\")\n",
        "\n",
        "# Now, we define check point conditions, \n",
        "# these checkpoints will save our model locally if there’s an improvement.\n",
        "filepath = \"/gdrive/My Drive/ted/ed-{epoch:02d}-{val_acc:.6f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor = 'val_acc', verbose = 1, save_best_only = True, mode = 'auto', save_weights_only = False)\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# fit x_train and y_train data to our model to train on them based on x_valid and y_valid \n",
        "history = model.fit(x_train2, y_train2, validation_data = (x_valid, y_valid), batch_size = batch_size, epochs = num_epochs, callbacks = callbacks_list)\n",
        "plot_history(history)\n",
        "\n",
        "\n",
        "ft = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
        "print(f\"[+] stop time for training >>> {ft}\")\n",
        "\n",
        "# evaluate model on the test data \n",
        "scores = model.evaluate(x_test, y_test, verbose = 0)\n",
        "tr_scores = model.evaluate(x_train, y_train, verbose = False)\n",
        "print(\"[+] Training Accuracy : {:.6f}\".format(tr_scores[1]))\n",
        "print(\"[+] Training Loss     : {:.6f}\".format(tr_scores[0]))\n",
        "te_scores = model.evaluate(x_test, y_test, verbose = False)\n",
        "print(\"[+] Testing Accuracy  : {:.6f}\".format(te_scores[1]))\n",
        "print(\"[+] Testing Loss      : {:.6f}\".format(tr_scores[0]))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[+] data size from csv >>>  (40000,)\n",
            "[+] label size from csv >>>  (40000,)\n",
            "\n",
            "[+] Excluding stopwords ...\n",
            "\n",
            "[+] Tokenized to Word indices as \n",
            "(40000,)\n",
            "\n",
            "[+] After padding data size is\n",
            "(40000, 20)\n",
            "\n",
            "[+] padded x_data >>> \n",
            "[[    0     0     0 ...   489  1014   433]\n",
            " [    0     0     0 ...  3327  2077   192]\n",
            " [    0     0     0 ...  3963  1598   143]\n",
            " ...\n",
            " [    0     0     0 ...  1390   165     2]\n",
            " [    0     0     0 ...  1135    19   336]\n",
            " [    0     0     0 ... 32852 32853 32854]]\n",
            "\n",
            "[+] Loading Glove Vectors ...\n",
            "\n",
            "[+] Loaded GloVe Vectors Successfully!\n",
            "\n",
            "[+] Embedding Matrix Generated with size >>>>  (32855, 50)\n",
            "\n",
            "[+] embedding matrix is \n",
            "[[ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [-0.067678    0.51832002  1.32599998 ... -0.65103     0.12924001\n",
            "   0.48723999]\n",
            " [ 0.11626     0.53896999 -0.39513999 ... -0.39061999 -0.10885\n",
            "   0.084513  ]\n",
            " ...\n",
            " [ 1.18879998  1.46720004 -0.99624002 ...  0.48704001  0.77978998\n",
            "   0.38242999]\n",
            " [ 0.013849   -0.54549998 -0.077683   ...  0.67878002  0.46202999\n",
            "   0.72376001]\n",
            " [ 0.023778   -0.80124003  0.1193     ... -1.06830001 -0.35703\n",
            "   0.21013001]]\n",
            "\n",
            "[+] Label Encoding Classes as \n",
            "{0: 'anger', 1: 'boredom', 2: 'empty', 3: 'enthusiasm', 4: 'fun', 5: 'happiness', 6: 'hate', 7: 'love', 8: 'neutral', 9: 'relief', 10: 'sadness', 11: 'surprise', 12: 'worry'}\n",
            "\n",
            "[+] One Hot Encoded class shape with size \n",
            "(40000, 13)\n",
            "\n",
            "[+] One hot encoded labels matrix\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 20, 50)            1642750   \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 20, 30)            1530      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 5, 30)             0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 5, 100)            52400     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 300)               150300    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 13)                3913      \n",
            "=================================================================\n",
            "Total params: 2,101,393\n",
            "Trainable params: 458,643\n",
            "Non-trainable params: 1,642,750\n",
            "_________________________________________________________________\n",
            "None\n",
            "Finished Preprocessing data ...\n",
            "[+] x_data shape >>>  (40000, 20)\n",
            "[+] y_data shape >>>  (40000, 13)\n",
            "[+] spliting data into training, testing set ...\n",
            "[+] start time to train >>> 2019-07-09 10:34:12\n",
            "Train on 29936 samples, validate on 64 samples\n",
            "Epoch 1/100\n",
            "29936/29936 [==============================] - 11s 383us/step - loss: 2.2553 - acc: 0.2319 - val_loss: 2.1104 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.31250, saving model to /gdrive/My Drive/ted/ed-01-0.312500.hdf5\n",
            "Epoch 2/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 2.1549 - acc: 0.2455 - val_loss: 2.0770 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.31250\n",
            "Epoch 3/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 2.1432 - acc: 0.2464 - val_loss: 2.0595 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.31250 to 0.32812, saving model to /gdrive/My Drive/ted/ed-03-0.328125.hdf5\n",
            "Epoch 4/100\n",
            "29936/29936 [==============================] - 10s 331us/step - loss: 2.1361 - acc: 0.2499 - val_loss: 2.0631 - val_acc: 0.2812\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.32812\n",
            "Epoch 5/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 2.1306 - acc: 0.2507 - val_loss: 2.0363 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.32812 to 0.34375, saving model to /gdrive/My Drive/ted/ed-05-0.343750.hdf5\n",
            "Epoch 6/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 2.1243 - acc: 0.2545 - val_loss: 2.0221 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.34375 to 0.35938, saving model to /gdrive/My Drive/ted/ed-06-0.359375.hdf5\n",
            "Epoch 7/100\n",
            "29936/29936 [==============================] - 10s 329us/step - loss: 2.1165 - acc: 0.2572 - val_loss: 2.0174 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.35938\n",
            "Epoch 8/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 2.1065 - acc: 0.2628 - val_loss: 1.9883 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.35938\n",
            "Epoch 9/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 2.0924 - acc: 0.2695 - val_loss: 1.9695 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.35938\n",
            "Epoch 10/100\n",
            "29936/29936 [==============================] - 10s 334us/step - loss: 2.0762 - acc: 0.2765 - val_loss: 1.9544 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.35938 to 0.37500, saving model to /gdrive/My Drive/ted/ed-10-0.375000.hdf5\n",
            "Epoch 11/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 2.0604 - acc: 0.2830 - val_loss: 1.9324 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.37500\n",
            "Epoch 12/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 2.0461 - acc: 0.2913 - val_loss: 1.9168 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.37500\n",
            "Epoch 13/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 2.0334 - acc: 0.2989 - val_loss: 1.8993 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.37500\n",
            "Epoch 14/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 2.0220 - acc: 0.3071 - val_loss: 1.8846 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.37500\n",
            "Epoch 15/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 2.0125 - acc: 0.3109 - val_loss: 1.8766 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.37500 to 0.39062, saving model to /gdrive/My Drive/ted/ed-15-0.390625.hdf5\n",
            "Epoch 16/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 2.0039 - acc: 0.3137 - val_loss: 1.8729 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.39062\n",
            "Epoch 17/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.9971 - acc: 0.3169 - val_loss: 1.8575 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.39062\n",
            "Epoch 18/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.9911 - acc: 0.3169 - val_loss: 1.8584 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.39062\n",
            "Epoch 19/100\n",
            "29936/29936 [==============================] - 10s 322us/step - loss: 1.9852 - acc: 0.3197 - val_loss: 1.8460 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.39062\n",
            "Epoch 20/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9808 - acc: 0.3211 - val_loss: 1.8339 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.39062\n",
            "Epoch 21/100\n",
            "29936/29936 [==============================] - 10s 322us/step - loss: 1.9756 - acc: 0.3241 - val_loss: 1.8277 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.39062\n",
            "Epoch 22/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9718 - acc: 0.3242 - val_loss: 1.8174 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.39062\n",
            "Epoch 23/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.9684 - acc: 0.3286 - val_loss: 1.8142 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.39062\n",
            "Epoch 24/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.9644 - acc: 0.3310 - val_loss: 1.7976 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.39062\n",
            "Epoch 25/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.9614 - acc: 0.3291 - val_loss: 1.7993 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.39062\n",
            "Epoch 26/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9580 - acc: 0.3308 - val_loss: 1.8014 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.39062 to 0.40625, saving model to /gdrive/My Drive/ted/ed-26-0.406250.hdf5\n",
            "Epoch 27/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.9546 - acc: 0.3337 - val_loss: 1.7938 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.40625\n",
            "Epoch 28/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.9521 - acc: 0.3338 - val_loss: 1.7977 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.40625\n",
            "Epoch 29/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.9500 - acc: 0.3340 - val_loss: 1.7972 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.40625\n",
            "Epoch 30/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9463 - acc: 0.3369 - val_loss: 1.7967 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.40625\n",
            "Epoch 31/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9440 - acc: 0.3382 - val_loss: 1.8046 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.40625\n",
            "Epoch 32/100\n",
            "29936/29936 [==============================] - 10s 320us/step - loss: 1.9412 - acc: 0.3377 - val_loss: 1.8018 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.40625\n",
            "Epoch 33/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.9388 - acc: 0.3373 - val_loss: 1.7781 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.40625\n",
            "Epoch 34/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.9362 - acc: 0.3402 - val_loss: 1.7762 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.40625\n",
            "Epoch 35/100\n",
            "29936/29936 [==============================] - 10s 322us/step - loss: 1.9343 - acc: 0.3413 - val_loss: 1.7821 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.40625\n",
            "Epoch 36/100\n",
            "29936/29936 [==============================] - 10s 332us/step - loss: 1.9324 - acc: 0.3400 - val_loss: 1.7792 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.40625\n",
            "Epoch 37/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.9301 - acc: 0.3411 - val_loss: 1.7658 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.40625\n",
            "Epoch 38/100\n",
            "29936/29936 [==============================] - 10s 319us/step - loss: 1.9286 - acc: 0.3435 - val_loss: 1.7679 - val_acc: 0.4375\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.40625 to 0.43750, saving model to /gdrive/My Drive/ted/ed-38-0.437500.hdf5\n",
            "Epoch 39/100\n",
            "29936/29936 [==============================] - 10s 320us/step - loss: 1.9263 - acc: 0.3433 - val_loss: 1.7700 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.43750\n",
            "Epoch 40/100\n",
            "29936/29936 [==============================] - 10s 320us/step - loss: 1.9239 - acc: 0.3445 - val_loss: 1.7801 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.43750\n",
            "Epoch 41/100\n",
            "29936/29936 [==============================] - 10s 319us/step - loss: 1.9219 - acc: 0.3443 - val_loss: 1.7770 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.43750\n",
            "Epoch 42/100\n",
            "29936/29936 [==============================] - 10s 331us/step - loss: 1.9202 - acc: 0.3437 - val_loss: 1.7839 - val_acc: 0.2969\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.43750\n",
            "Epoch 43/100\n",
            "29936/29936 [==============================] - 10s 319us/step - loss: 1.9181 - acc: 0.3460 - val_loss: 1.7776 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.43750\n",
            "Epoch 44/100\n",
            "29936/29936 [==============================] - 10s 322us/step - loss: 1.9165 - acc: 0.3459 - val_loss: 1.7814 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.43750\n",
            "Epoch 45/100\n",
            "29936/29936 [==============================] - 10s 319us/step - loss: 1.9148 - acc: 0.3474 - val_loss: 1.7714 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.43750\n",
            "Epoch 46/100\n",
            "29936/29936 [==============================] - 10s 321us/step - loss: 1.9134 - acc: 0.3469 - val_loss: 1.7952 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.43750\n",
            "Epoch 47/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9116 - acc: 0.3473 - val_loss: 1.7724 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.43750\n",
            "Epoch 48/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.9098 - acc: 0.3493 - val_loss: 1.7716 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.43750\n",
            "Epoch 49/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.9079 - acc: 0.3492 - val_loss: 1.7563 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.43750\n",
            "Epoch 50/100\n",
            "29936/29936 [==============================] - 10s 321us/step - loss: 1.9060 - acc: 0.3484 - val_loss: 1.7577 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.43750\n",
            "Epoch 51/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.9052 - acc: 0.3493 - val_loss: 1.7827 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.43750\n",
            "Epoch 52/100\n",
            "29936/29936 [==============================] - 10s 327us/step - loss: 1.9042 - acc: 0.3520 - val_loss: 1.7629 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.43750\n",
            "Epoch 53/100\n",
            "29936/29936 [==============================] - 10s 329us/step - loss: 1.9021 - acc: 0.3510 - val_loss: 1.7605 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.43750\n",
            "Epoch 54/100\n",
            "29936/29936 [==============================] - 10s 331us/step - loss: 1.9015 - acc: 0.3513 - val_loss: 1.7839 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.43750\n",
            "Epoch 55/100\n",
            "29936/29936 [==============================] - 10s 329us/step - loss: 1.8996 - acc: 0.3508 - val_loss: 1.7709 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.43750\n",
            "Epoch 56/100\n",
            "29936/29936 [==============================] - 10s 329us/step - loss: 1.8986 - acc: 0.3542 - val_loss: 1.7709 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.43750\n",
            "Epoch 57/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.8973 - acc: 0.3527 - val_loss: 1.7713 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.43750\n",
            "Epoch 58/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 1.8957 - acc: 0.3552 - val_loss: 1.7797 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.43750\n",
            "Epoch 59/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 1.8944 - acc: 0.3538 - val_loss: 1.7862 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.43750\n",
            "Epoch 60/100\n",
            "29936/29936 [==============================] - 10s 329us/step - loss: 1.8933 - acc: 0.3554 - val_loss: 1.7826 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.43750\n",
            "Epoch 61/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 1.8921 - acc: 0.3545 - val_loss: 1.7847 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.43750\n",
            "Epoch 62/100\n",
            "29936/29936 [==============================] - 10s 331us/step - loss: 1.8907 - acc: 0.3549 - val_loss: 1.7842 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.43750\n",
            "Epoch 63/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.8900 - acc: 0.3566 - val_loss: 1.7786 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.43750\n",
            "Epoch 64/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.8875 - acc: 0.3580 - val_loss: 1.7805 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.43750\n",
            "Epoch 65/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 1.8870 - acc: 0.3575 - val_loss: 1.7817 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.43750\n",
            "Epoch 66/100\n",
            "29936/29936 [==============================] - 10s 335us/step - loss: 1.8852 - acc: 0.3595 - val_loss: 1.7730 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.43750\n",
            "Epoch 67/100\n",
            "29936/29936 [==============================] - 10s 332us/step - loss: 1.8846 - acc: 0.3579 - val_loss: 1.7704 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.43750\n",
            "Epoch 68/100\n",
            "29936/29936 [==============================] - 10s 333us/step - loss: 1.8837 - acc: 0.3587 - val_loss: 1.7941 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.43750\n",
            "Epoch 69/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.8822 - acc: 0.3595 - val_loss: 1.7744 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.43750\n",
            "Epoch 70/100\n",
            "29936/29936 [==============================] - 10s 327us/step - loss: 1.8807 - acc: 0.3585 - val_loss: 1.7996 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.43750\n",
            "Epoch 71/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8799 - acc: 0.3584 - val_loss: 1.7735 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.43750\n",
            "Epoch 72/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8785 - acc: 0.3606 - val_loss: 1.7758 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.43750\n",
            "Epoch 73/100\n",
            "29936/29936 [==============================] - 10s 330us/step - loss: 1.8774 - acc: 0.3609 - val_loss: 1.7826 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.43750\n",
            "Epoch 74/100\n",
            "29936/29936 [==============================] - 10s 333us/step - loss: 1.8764 - acc: 0.3610 - val_loss: 1.7904 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.43750\n",
            "Epoch 75/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8750 - acc: 0.3595 - val_loss: 1.8018 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.43750\n",
            "Epoch 76/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8736 - acc: 0.3625 - val_loss: 1.7802 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.43750\n",
            "Epoch 77/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8733 - acc: 0.3600 - val_loss: 1.7800 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.43750\n",
            "Epoch 78/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.8719 - acc: 0.3610 - val_loss: 1.8091 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.43750\n",
            "Epoch 79/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.8711 - acc: 0.3624 - val_loss: 1.7866 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.43750\n",
            "Epoch 80/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8700 - acc: 0.3629 - val_loss: 1.7748 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.43750\n",
            "Epoch 81/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8685 - acc: 0.3625 - val_loss: 1.7763 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.43750\n",
            "Epoch 82/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.8675 - acc: 0.3643 - val_loss: 1.7802 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.43750\n",
            "Epoch 83/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8658 - acc: 0.3648 - val_loss: 1.7914 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.43750\n",
            "Epoch 84/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8647 - acc: 0.3633 - val_loss: 1.7820 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.43750\n",
            "Epoch 85/100\n",
            "29936/29936 [==============================] - 10s 327us/step - loss: 1.8640 - acc: 0.3661 - val_loss: 1.7843 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.43750\n",
            "Epoch 86/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8627 - acc: 0.3658 - val_loss: 1.7831 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.43750\n",
            "Epoch 87/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8614 - acc: 0.3683 - val_loss: 1.7777 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.43750\n",
            "Epoch 88/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.8595 - acc: 0.3676 - val_loss: 1.8010 - val_acc: 0.3750\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.43750\n",
            "Epoch 89/100\n",
            "29936/29936 [==============================] - 10s 320us/step - loss: 1.8592 - acc: 0.3673 - val_loss: 1.7868 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.43750\n",
            "Epoch 90/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.8584 - acc: 0.3673 - val_loss: 1.7961 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.43750\n",
            "Epoch 91/100\n",
            "29936/29936 [==============================] - 10s 327us/step - loss: 1.8564 - acc: 0.3675 - val_loss: 1.8074 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.43750\n",
            "Epoch 92/100\n",
            "29936/29936 [==============================] - 10s 322us/step - loss: 1.8554 - acc: 0.3689 - val_loss: 1.7994 - val_acc: 0.3281\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.43750\n",
            "Epoch 93/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8541 - acc: 0.3706 - val_loss: 1.7911 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.43750\n",
            "Epoch 94/100\n",
            "29936/29936 [==============================] - 10s 323us/step - loss: 1.8531 - acc: 0.3691 - val_loss: 1.7944 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.43750\n",
            "Epoch 95/100\n",
            "29936/29936 [==============================] - 10s 321us/step - loss: 1.8514 - acc: 0.3682 - val_loss: 1.7820 - val_acc: 0.4219\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.43750\n",
            "Epoch 96/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8513 - acc: 0.3699 - val_loss: 1.8093 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.43750\n",
            "Epoch 97/100\n",
            "29936/29936 [==============================] - 10s 325us/step - loss: 1.8494 - acc: 0.3695 - val_loss: 1.8077 - val_acc: 0.3594\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.43750\n",
            "Epoch 98/100\n",
            "29936/29936 [==============================] - 10s 324us/step - loss: 1.8482 - acc: 0.3707 - val_loss: 1.7827 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.43750\n",
            "Epoch 99/100\n",
            "29936/29936 [==============================] - 10s 328us/step - loss: 1.8474 - acc: 0.3702 - val_loss: 1.7938 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.43750\n",
            "Epoch 100/100\n",
            "29936/29936 [==============================] - 10s 326us/step - loss: 1.8454 - acc: 0.3736 - val_loss: 1.7907 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.43750\n",
            "[+] stop time for training >>> 2019-07-09 10:50:32\n",
            "[+] Training Accuracy : 0.376200\n",
            "[+] Training Loss     : 1.838973\n",
            "[+] Testing Accuracy  : 0.334900\n",
            "[+] Testing Loss      : 1.838973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAFACAYAAACoSyokAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBWQhFkIIYQQQogKSMIshBBCCCFEBf4fWoepXZTKulIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFdfk3b9FC_",
        "colab_type": "text"
      },
      "source": [
        "# **GETTING EMOTIONS OF OUR TEXT**\n",
        "*`predicting using our model`*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Bi2Ffu9EwA",
        "colab_type": "code",
        "outputId": "71ad27ff-30c8-46cd-8754-404cd3ab6979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "import numpy as np, emojis, nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def textToInput(x):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    new_stop_words = set(stop_words)\n",
        "    for s in stop_words:\n",
        "        new_stop_words.add(s.replace('\\'',''))\n",
        "        pass\n",
        "    stop_words = new_stop_words\n",
        "    base_filters = '\\n\\t!\"#$%&()*+,-./:;<=>?[\\]^_`{|}~ '\n",
        "    word_sequences = []\n",
        "    for i in x:\n",
        "        i = str(i)\n",
        "        i = i.replace('\\'', '')\n",
        "        newlist = [x for x in text_to_word_sequence(i,filters = base_filters, lower = True) if not x.startswith(\"@\")]\n",
        "        filtered_sentence = [w for w in newlist if not w in stop_words] \n",
        "        word_sequences.append(filtered_sentence)\n",
        "        pass\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(word_sequences)\n",
        "    word_indices = tokenizer.texts_to_sequences(word_sequences)\n",
        "    word_index = tokenizer.word_index\n",
        "    x_data = pad_sequences(word_indices, maxlen=20)\n",
        "    print(\"\\n[+] NN input is <<< word indices , 20 length of each sequence after padded >>>: \")\n",
        "    print(x_data)\n",
        "    print()\n",
        "    return x_data\n",
        "\n",
        "\n",
        "emojies = {\n",
        "         0: ':angry:', 1: ':expressionless:', 2: ':no_mouth:', 3: ':heart_eyes:', \n",
        "         4: ':smile:', 5: ':blush:', 6: ':-1:', 7: ':heart:' , 8: ':neutral_face:',\n",
        "         9: ':sweat_smile:', 10: ':pensive:', 11: ':anguished:' , 12: ':worried:'\n",
        "        }\n",
        "\n",
        "text = [\"can you see the sky, there is a little hole in there!\", \"unknown sense your toy inside her room before his mother come into me\"]\n",
        "text_mat = np.asarray(text)\n",
        "NNInput = textToInput(text_mat)\n",
        "model = load_model('/gdrive/My Drive/ted/ted.hdf5')\n",
        "prediction = model.predict(NNInput)\n",
        "\n",
        "# -----------\n",
        "# prediction:\n",
        "# -----------\n",
        "# eg for above text: \n",
        "# we'll choose the maximum prob to show the emotion of our text\n",
        "# we can also show every prob to the user in order to analysis his/her text!\n",
        "\n",
        "# [[2.0208173e-03 2.9407262e-03 1.9351950e-02 3.1280160e-02 4.1118231e-02\n",
        "#   2.1743497e-01 5.4602097e-03 8.7387480e-02 2.2649333e-01 6.2394161e-02\n",
        "#   8.3200514e-02 7.1597643e-02 1.4931984e-01]\n",
        "#  [3.5918120e-04 5.1850826e-04 3.2069117e-03 8.3221346e-03 3.3233363e-02\n",
        "#   1.1314391e-01 1.0906422e-03 6.4181775e-01 6.9848925e-02 1.5428309e-02\n",
        "#   2.2029646e-02 3.4768842e-02 5.6231819e-02]]\n",
        "\n",
        "for i in prediction:\n",
        "    print(emojis.encode(f' you are  {emojies[np.argmax(i)]}'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "[+] NN input is <<< word indices , 20 length of each sequence after padded >>>: \n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  2  3  4]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  6  7  8  9 10 11]]\n",
            "\n",
            " you are  😐\n",
            " you are  ❤️\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}