

# create a penetration testing framework like a password cracker(a decrypter) using ANN  
# decrypting/cracking passwords using ANN idea ; start with a genetic algorithm to learn to print hello world
'''decrypting cookies, hashed pswds & bit,SSLs(packet capture app in my phone) and sessions with/without ANN(use python cipher books) 
   create a cipher decoder app(a bruteforcer script from scratch like crunch) using python for my own purposes(decode all ciphers and hashing algos; 
   use python secret cipher books in WOShelf); hack soroush app break the ssl & AES & show them using wireshark concept & turn RSA into bits with/without using ANN'''
# getting start with ANN(like genetic algo and virus creation) in everything like penjabing and vocfu life-style for the next level of my life => python + raspi + AI(ANN-tensorflow) + vocfu bag
# build ur icfu SlAb(wocfulab) in csr with ur team using AI + use ANN ML DL in MAN project to take them down + build AI projects and cvs with ICFU team(u know who they R!)
# TOKNOW: ai health itself (how it know what is it doing like human? try to understand west world movie!)
# TOKNOW: lstm vs self-awareness and human memory
# TOKNOW: hyper dimensional computnig theaory for AI memory(self-awareness) alternatives to pytorch/tensorflow/keras traditional ann(RIL, SUPL, USUPL)
# TOKNOW: ANI-AGI-ASI , all about NLP and its vectors , unSED (unsupervised-emotion-detection) and SED (supervised-emotion-detection) using RNNs
# TOKNOW: openAI RL ppo vs deepNeuroevolution algo vs Hyperdimensional Computing Theory(HBV) 

# TENET: AI & ANN in everything like making ur own anti-virus(statistics, dynamic and heuristic knowledge) stronger than windows defender using AI(AI must detect the all virus signatures and learn to find them)
# TODO: FUD malware creation using AI(pytorch/tensorflow/keras/numpy/matplotlib on google colab) and also malware detection/classification using machine learning
# TODO: AI based(pytorch/tensorflow/keras/numpy/scipy/matplotlib/opencv/scikit-learn on google colab) airdrone to do some IOT stuff using our own blockchain system (solidity smart-contract) and serverless architecture using dart api(saas, faas, baas, paas) to run airline transportation protocol app(vue-electron || flutter) built on top of CSR
# TODO: build ur virus using AI like so: first it'll use ANN to create a cryptographic network(use all articles and pdfs in UT folder for ANN and machine learning) for its ransomware encryption(cL34n 3v3RytH!n9) also for bypassing technique it'll analyze the AV behavior to know the AV detection algorithms such as signature scanning to behavior the opposite of it and hide itself from AV; the virus has two main parts in which the AV behavior analyzing is the first part for bypassing and the main content of virus(ransomware, backdoor) is the second part which will run after a success analyzing job in first part! also it'll use the blockchain for storing vic and AV infos after successfull detection
# TODO: use AI in ur MAN project like backdoor creation using AI to bypass et; cause MAN proj is all u got to destroy them
# TODO: think about hacking using ANN not a shity hack like iranian hack so build all types of ANN from scratch with ur own language(ur own compiler like dart) and math(derivation) concepts then put it on wocfulab 
# TODO: think about to create a bot or anything for rana to answer every fucking question of her instead of me using AI ANN ML DL
# TODO: build ur own framework based on AI and ANN ; you can see the wocfulab.todo
# TODO: spoil prevention idea for movies using AI (watch what ur unconscious want! the Agent must access to ur unconscious to surprise u with scenes that ur brain want to see or get what u want to see!)
# TODO: solve Artificial Super Intelligence(ASI) dangerous and problems using a two way neuron wave collector which it can put/read thoughts in/from your mind like Doublegainer or Password_Cracker idea(for that we have to build a security protocl like our ASI must prevent those bad thoughts like killing or wars from being happen or no one can hear other thoughts) | implement the abstract of inertia or force for our ASI
# TODO: build a nano robots using Reinforement Learning openAI PPO algo or Deep Neuroevolution or Hyperdimensional Computing theory(HBV) to put them inside our body for two way neuron wave collector device to interact with our neuron waves and send their info waves or thoughts waves to our device for ASI ops 
# TODO: create a script for sql injection using ANN; use GyoiThon framework and related below github links ; also build a graph based todo app on top of AI using ~WOCAPP idea(the agent must understand you and tell you what job you have to do it now according to ur mood)
# TODO: create a tool to decrypt all passwords algos using ANN(encrypt and crack all algos with ANN) ; create an agent to crack all apps and build the keygen
# TODO: build an AI/RL based operating system called AIVO for my hacking and jabing purposes; os create the botnet itself; build a os to control the baseband and more things
# TODO: Dapp creation like D-tube and steemit cuase it's new using below links and AI concepts: https://hackernoon.com/blockchain/home, https://storj.io/, https://ipfs.io/, https://github.com/ubirch/, https://sia.tech/, https://maidsafe.net/, https://filecoin.io/, https://www.mix-blockchain.org/, https://eos.io/
# TODO: create a system for collage students those who their state is a little bit close to conditional situation but they passed all their courses in that semester; so we can help them by this system in order to give them either a conditional state or passed state by taking a process time for investigating and collecting big data about their manner, behavior, personality, efforts for each semester and other majors of their past life then mine those data using AI and let the AI say us what'll happen! 
# TODO: app for mafia game using AI => it's a bot in which two main roles can randomly choose for the bot : the mafia and the police, if the bot is mafia then it can learn from the behavior of other palyers(mafia, police, nurse and etc...) in first rounds also it can learn the behavior of who are/were mafia too in order to play the best role as a mafia and kill other polices; if it's not mafia the it can learn from the behavior of other polices in couple of last rounds to play the best role as a police and kill the mafia. note: (there is no way to defeat this bot and kill it as a mafia or a police)
# TODO: build a suicide bot to produce a noisy voice for ur vic according to the manner, behaviour, past life of vic and all emotions and sences that he/she got before using AI to send a disappinment waves to his ears
# TODO: build smt like stem in upgrade movie and AI connect game(its header is below) technology using RL algos to read the mind(implement it in python or nodejs)
# TODO: get your hand dirty in dapp(Ethereum dApp) creation and use blockchain in ur vocfu life style in ur wocfulab which is on top of AI
# TODO: build ur own cafe(use javad information theory and maximal structure idea) in which u can order anything by thinking through a device(a two way neuron wave collector which it can put thoughts in your mind through its wave) based on blockchain(smart contract) iot(raspi and vocfu bag) and ml(data science and csr ; like using cnn for the surveillance camera of cafee)
# TODO: build a civilization lab like west world using RL/ES/DN/simulation then push it in ur wocfulab ; also build THEND idea which is about appdev using AI and client brain(the client think with our device then the whole directory of the app and its structure with the best optimality solution(we can also use our own structure ; a compiler better than the MVC style which combine our logic and all queris in our ui file with our own language) will build for him/her which is either a web and desktop or mobile app ===>> ui framework for all browsers like vuetify or smt like firebase[bricks and typeacid force code] with ui paintintg idea for app ui)
# TODO: extend THEND idea in such a way that when you give a picture of your env to it it'll give the whole code of that env to you! 
# TODO: AI based fia/museum smart music player feature using hadoop and parallelism concepts for its big data and data mining(WOCAB idea using nuxt + electron + pytorch + flutter + adobexd for ui) then push it on ur wocfulab
# TODO: build an agents as a writer who wirtes drama for theater and give this drama to other agents to take part in it and play their own roles and acts better than human
# TODO: build an app/hardware to change the voulume of the music accroding to the env noise using AI
# TODO: backdoor creation using ayncio , threading , raw socket , vocfu bag raspi hak5 tools and pytorch(tensorflow) RL blockchain IOT
# TODO: Orcus scenarios bypassing using DoubleAgent(see below links for backdoor creation or backdoor bypassing using RL in pytorch)
# TODO: all about tensors , graph & tree pathfinding and matrix/tensors in NLA and implement blockchain and iot on raspi(openwrt.org) using pytorch(AI) and a tpu/intel neural stick with a 3D printer on top of data science vs csr
# TODO: malware detection using adversarial NN and malware training using DQN(DRL algos based on MDP framework and belman equation)
# TODO: create a code to destroy and burn the hard and cpu using pytorch , assembly[bash terminal(nasm)] and python rop libs like struct(focus on CSA and OS concepts inside bash terminal through cmd)
# TODO: maze , Tic-tac-toe , 2048 , sudoku , N-queens , RL/GA(ES/DN) and data science in health-care , rasol PDE article , smart notebook idea using ANN , poerty agent , sweetheartbot , mafia(implement on raspi stuff tools with blockchain on top of pytorch)
# TODO: build an WILAM application on top of ML with python flask backend using keras and pytorch and nuxt electron express flutter dart package for its ui(look wocfulab.todo for more details) also buil AI LCMS(javad idea) using ML algos
# TODO: javad maximal structure , echo talar and information theory RL ideas(article) to reinforce the NN to write our virus code based on given input information
# TODO: red square , NQAgent.py , sudokuAgent.py , connect game , telbot using DRL(DQN)/RNNLM/NLP , ai.py ideas , wocfulab.todo on top of these goals(data science vs csr)
# TODO: implement DQN improvement algorithms and RL algos on MDP based env(DQN.pdf inside AI folder) also use genetic algo(Evolutionary Strategy/Deep Neuroevolution) PPO as an alternative for RL and other RL algos along with probability/statistics concepts
# TODO: pygame|arcade|tkinter|openAIgym , CNN VGG and opencv , GAN , SL/USL , DCGAN , DRL(DQN(DNN,DQL)|ES|PPO|DN|GA) , RNNLM , NLP(NLU) , RNN(LSTM)
# TOKNOW: ES/GA/DN is an alternative for RL and some algo like DDQN is an improvement algo for DQN(DRL) and other alog like PPO is a RL algo and all NN types like CNN GAN RNN VGG NLP are supervised learning type of ML
# TOKNOW: Q-learning algo is a solution to solve the RL issues on MDP env using bellman equation and DQN(DRL/DQL) is a combination of Q-learning algo with neural network
# TOKNOW: for all uis of all ur apps just use adobexd and run all ur codes in a parallelism env in python parallel programming
'''
use all articles and pdfs in UT folder for ANN and machine learning + create packer/encoder/crypter using ANN + use twitter db to build my dataset for NN in order to create a bot to comment on every post intelligently 
hack all the things(idea) using ANN[tensorflow ANN hacking + raspi] => write ur ideas with above links & AI DL ML ANN concepts
AI ML DL A(D)NN in penetration testing(read/use GyoiThon framework code and use BrainDamage code)
below ideas + wISn tech idea(write it down[hafez fall]) + my idea about blockchain vs cryptography & quantum computing (search on youtube videos)
connect to net without ISP direct connecting using ANN idea(DRL(DQN)) and other wifi radio waves; is more like MAC spoofing but what about the DNS(CCNA-Wireless-** **-Official-Cert-Guide pdfs to know the behaviour of 802.11 waves & signal course & use coding theory in this project & know all about ISP)
getting the neuron waves using quantum tech(quantum computer) or a device which can collect the neuron waves data without any sticking ways for my password cracking idea 
hack pos machine using sp reverse shell concept,teensy payload to upload client file in there and linux python or assembly lang or using ANN
hack trrafic light using my tools or using ANN
all iot projects with nodemcu raspi & raspi ANN tensorflow in hacking & python telbot to scraping websites & upload python code on pythonanywhere
eye ball movement to control computer screen using tensorflow RNN(above links)
nwd pedigree idea tech(search on youtube) => they can see the gift from their parents and we have to collect this big data and mine them to find a pattern between them using AI for creating a virus for a specific generation 
hack all the things(idea) using ANN[tensorflow ANN hacking + raspi] => write ur ideas with above links & AI DL ML ANN concepts
AI ML DL A(D)NN in penetration testing(read/use GyoiThon framework code and use BrainDamage code)
my other ideas in ai.py + wISn tech idea(write it down) + my idea about blockchain vs cryptography & quantum computing (youtube videos)
connect to net without ISP direct connecting using ANN idea and other wifi radios(CCNA-Wireless-** **-Official-Cert-Guide pdfs to know the behaviour of 802.11 waves & signal course)
getting the neuron waves using quantum tech(quantum computer) or a device which can collect the neuron waves data without any sticking ways for my password cracking idea 
hack pos machine using sp reverse shell concept,teensy payload to upload client file in there and linux python or assembly lang or using ANN
hack trrafic light using my tools or using ANN
eye ball movement to control computer screen using tensorflow RNN
dapp creation using ethereum stuffs like truffle and solidity
'''

# ---------------------------
# CONNECT GAME IMPLEMENTATION
# ---------------------------
'''
connect game in python.
it'll listen to the player which he/she said a SilentPhonetic
then this bot will give us a word using DRL model
in which its two first letter match with the player's letters
that he/she just said. after making a connection to someone
this bot will counting in a reverse manner and say the word. 
this bot should connect to other players in such a way that 
defeat the player in first round!  
we need a pattern to read other players mind to know their
manners, behaviours and thoughts with this bot
we need to get the players voice then convert them into words
'''


'''
SOURCES & TUTS:
https://colab.research.google.com/drive/1p51HFDExl7XagWfSQE5leDQwBC6I_e3D
https://colab.research.google.com/drive/14I-31WuynLg1B0RQHWwwRgBmqTlDgkwV
https://www.microsoft.com/developerblog/2015/11/29/emotion-detection-and-recognition-from-text-using-deep-learning/
https://www.psychologytoday.com/us/blog/the-future-brain/201904/neuroscientists-transform-brain-activity-speech-ai
https://www.theregister.co.uk/2019/01/30/ai_brain_reader/
https://www.wired.com/story/ml-brain-boost/
https://www.infoq.com/news/2019/03/deep-learning-speech-brain/
https://newatlas.com/brain-signals-into-speech-algorithm/58253/
https://www.digitaltrends.com/cool-tech/ai-thought-to-speech/
https://towardsdatascience.com/a-beginners-guide-to-brain-computer-interface-and-convolutional-neural-networks-9f35bd4af948
https://towardsdatascience.com/from-brain-waves-to-arm-movements-with-deep-learning-an-introduction-3c2a8b535ece
https://www.sciencemag.org/news/2019/01/artificial-intelligence-turns-brain-activity-speech
https://eng.umd.edu/release/helping-robots-remember-hyperdimensional-computing-theory-could-change-the-way-ai-works
https://www.futurity.org/brain-computer-interface-robotic-arm-2088502/
https://www.futurity.org/deepfake-videos-detection-computer-vision-2088302/
https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c
https://scialert.net/fulltextmobile/?doi=jas.2007.2812.2817
https://www.techworld.com/tech-innovation/what-is-catastrophic-forgetting-how-does-it-affect-ai-development-3687007/
https://medium.com/@AIerusalem/catastrophic-importance-of-catastrophic-forgetting-c1c2a245a662
https://www.forbes.com/sites/federicoguerrini/2017/05/08/new-deep-learning-system-allows-ai-to-solve-catastrophic-forgetting-problem/
https://www.pnas.org/content/114/13/3521
https://qz.com/933223/deepmind-developed-an-artificial-intelligence-algorithm-to-tackle-catastrophic-forgetting/
https://www.telegraph.co.uk/technology/2017/03/15/googles-deepmind-ai-learns-like-human-overcome-catastrophic/
https://indico.io/blog/recognizing-emotion-in-text-machine-learning-no-code/
http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/
https://www.kdnuggets.com/2018/08/emotion-sentiment-analysis-practitioners-guide-nlp-5.html
https://www.tensorflow.org/tutorials/non-ml/pdes
https://csirtgadgets.com/commits/2018/8/17/hunting-for-malicious-connections-using-python-and-tensorflow
https://medium.com/@radicalrafi/untitled-document-md-87f85d658a9a
https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing
https://dzone.com/articles/malware-detection-with-convolutional-neural-networ
https://github.com/AFAgarap/malware-classification
https://dzone.com/articles/malware-detection-with-convolutional-neural-networ
https://github.com/AFAgarap/malware-classification
https://www.evilsocket.net/2019/05/22/How-to-create-a-Malware-detection-system-with-Machine-Learning/
https://towardsdatascience.com/understanding-generative-adversarial-networks-4dafc963f2ef
https://becominghuman.ai/genetic-algorithm-for-reinforcement-learning-a38a5612c4dc
https://pastebin.com/ZZmSNaHX
https://www.analytics-link.com/single-post/2019/02/14/Password-Cracking-with-a-Genetic-Algorithm
https://onlinelibrary.wiley.com/doi/abs/10.1002/cjce.23350
https://towardsdatascience.com/evolution-of-a-salesman-a-complete-genetic-algorithm-tutorial-for-python-6fe5d2b3ca35
https://www.youtube.com/watch?v=Pls_q2aQzHg
https://medium.com/predict/a-step-towards-agi-the-story-of-alphago-e0fafd83e6b9
https://deepmind.com/research/alphago/
https://towardsdatascience.com/gas-and-nns-6a41f1e8146d
https://towardsdatascience.com/artificial-neural-networks-optimization-using-genetic-algorithm-with-python-1fe8ed17733e
https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164
https://medium.com/analytics-vidhya/the-scuffle-between-two-algorithms-neural-network-vs-support-vector-machine-16abe0eb4181
https://www.youtube.com/watch?v=1NxnPkZM9bc
https://www.youtube.com/watch?v=j_pJmXJwMLA
https://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://skymind.ai/wiki/lstm
https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0
https://medium.com/datadriveninvestor/recurrent-neural-networks-and-long-short-term-memory-5d17bdbdfc00
https://www.analyticsvidhya.com/blog/2018/07/evolutionary-algorithm-perfect-useful-alternative-neural-network/
https://www.innovationtoronto.com/2019/02/engineers-create-a-robot-that-can-imagine-itself/
https://towardsdatascience.com/recurrent-neural-networks-and-lstm-4b601dd822a5
https://techxplore.com/news/2019-05-recreate-human-like-machines.html
https://techxplore.com/news/2019-05-robots-hyperdimensional-theory-ai.html
https://techxplore.com/news/2019-05-ai-taught-video-game-beatinghumans.html
https://robotics.sciencemag.org/content/4/30/eaaw6736
https://www.sciencedaily.com/releases/2019/05/190515165455.htm
https://techxplore.com/news/2019-05-algorithm-people-pictures-videos-faster.html
https://techxplore.com/news/2018-10-developmental-framework-robots-optimize-hyper-parameters.html
https://techxplore.com/news/2018-10-brain-inspired-algorithm-ai-multitask.html
https://techxplore.com/news/2019-03-memory-approach-enable-lifelong.html
https://www.sri.com/work/projects/artificial-intelligence-system-continually-learns
https://techxplore.com/news/2019-03-memory-approach-enable-lifelong.html
https://techxplore.com/news/2019-05-framework-artificial-intelligence.html
https://phys.org/news/2019-06-machine-sensors.html
https://www.youtube.com/watch?v=zUCoxhExe0o
https://www.nanowerk.com/news2/robotics/newsid=52814.php?utm_source=feedblitz&utm_medium=FeedBlitzRss&utm_campaign=nanowerkemergingtechnologiesnews
https://www.scitecheuropa.eu/hyperdimensional-computing-theory-robots-memory/95060/
https://bigthink.com/technology-innovation/discovery-ai-robots-create-memories
https://www.innovationtoronto.com/2019/05/hyperdimensional-computing-theory-could-change-the-way-ai-works-by-helping-robots-to-remember/
https://thenextweb.com/artificial-intelligence/2019/05/17/hyperdimensional-computing-theory-could-lead-to-ai-with-memories-and-reflexes/
https://www.allaboutcircuits.com/news/artificial-intelligence-memory-basics-of-hyperdimensional-computing/
https://enlight.nyc/projects/neural-network/
https://www.youtube.com/watch?v=2rDkQWi-RA4&t=666s
https://www.youtube.com/watch?v=DWsJc1xnOZo
https://www.theverge.com/2019/1/28/18194816/ai-artificial-intelligence-issue
https://app.sndbox.com/login
https://youtu.be/kqSzLo9fenk
https://towardsdatascience.com/an-easy-introduction-to-unsupervised-learning-with-4-basic-techniques-897cb81979fd
https://blog.openai.com/evolution-strategies/
https://medium.com/@benjamin.phillips22/evolution-strategies-as-a-scalable-alternative-to-reinforcement-learning-paper-summary-691161b52ddd
https://flyyufelix.github.io/2018/06/11/sonic-rl.html
https://towardsdatascience.com/deploying-a-keras-deep-learning-model-as-a-web-application-in-p-fc0f2354a7ff
https://towardsdatascience.com/deep-neuroevolution-genetic-algorithms-are-a-competitive-alternative-for-training-deep-neural-822bfe3291f5
https://github.com/topics/unsupervised-learning
https://github.com/mindis/002_MachineLearning_eBook
https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning | https://pytorch.org
https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/
https://www.youtube.com/watch?v=nSxaG_Kjw_w 
https://datascience.stackexchange.com/questions/38845/what-is-the-relationship-between-mdp-and-rl
https://stats.stackexchange.com/questions/34357/q-learning-in-a-stochastic-environment
https://github.com/CodeReclaimers/neat-python/tree/master/examples/single-pole-balancing
https://www.cs.cmu.edu/~avrim/courses.html | https://www.cs.cmu.edu/~avrim/
https://stats.stackexchange.com/questions/336974/when-are-monte-carlo-methods-preferred-over-temporal-difference-ones
https://datascience.stackexchange.com/questions/26471/is-my-understanding-of-on-policy-and-off-policy-td-algorithms-correct
https://www.quora.com/Does-Deep-Q-learning-use-Monte-Carlo-tree-search
https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4
https://becominghuman.ai/machines-demonstrate-self-awareness-8bd08ceb1694
https://github.com/13o-bbr-bbq/machine_learning_security/tree/master/DeepExploit
https://github.com/cchio/deep-pwning
https://github.com/13o-bbr-bbq/machine_learning_security
https://github.com/gyoisamurai/GyoiThon
https://github.com/src-d/awesome-machine-learning-on-source-code
https://github.com/jivoi/awesome-ml-for-cybersecurity
https://github.com/zhexxian/From-Machine-Learning-To-Zero-Day-Exploits
https://github.com/philipperemy/deep-learning-bitcoin
https://github.com/RandomAdversary/Awesome-AI-Security
https://github.com/Hack-with-Github/Awesome-Hacking
https://www.python-course.eu/graphs_python.php
https://www.jeremyjordan.me/rl-learning-methods/
https://en.wikibooks.org/wiki/Artificial_Intelligence/AI_Agents_and_their_Environments
https://www.youtube.com/watch?v=CIfsB_EYsVI
https://www.youtube.com/watch?v=IxQtK2SjWWM
https://www.youtube.com/watch?v=lvoHnicueoE&t=88s
https://www.youtube.com/watch?v=OYhFoMySoVs
https://www.youtube.com/watch?v=fIKkhoI1kF4
https://www.youtube.com/watch?v=iLNHVwSu9EA
https://www.youtube.com/watch?v=C14VDpGAbSE
https://www.youtube.com/watch?v=1g1HCYTX3Rg
https://www.youtube.com/watch?v=miPyFmr4iCc
https://www.quora.com/What-are-the-different-types-of-Machine-Learning-Algorithms
https://stackoverflow.com/questions/26182980/can-anyone-give-a-real-life-example-of-supervised-learning-and-unsupervised-lear
https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d
https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/
https://github.com/Kyushik/DRL
https://github.com/Kaixhin/Rainbow
http://outlace.com/rlpart3.html
https://github.com/keras-rl/keras-rl
https://github.com/endgameinc/gym-malware
https://github.com/keras-team/keras/tree/master/examples
https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html
https://pastebin.com/ZZmSNaHX
https://gym.openai.com/evaluations/eval_ujFWHmoqSniDh8cErKCVpA/
https://github.com/mymultiverse/GeneticAlgo_OpenAIGymCartPole
https://github.com/HackerShackOfficial/OpenAI-NEAT
https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0
https://medium.com/@xbno/openai-gym-and-evolutionary-models-5232fd94226d
https://becominghuman.ai/genetic-algorithm-for-reinforcement-learning-a38a5612c4dc
https://github.com/vmayoral/basic_reinforcement_learning/blob/master/tutorial5/evolutionary-ann-cartpole.py
https://github.com/topics/genetic-algorithm?l=python&o=desc&s=stars
https://dzone.com/articles/beating-atari-games-with-openais-evolutionary-stra
https://www.youtube.com/watch?v=RznKVRTFkBY&list=PLZbbT5o_s2xrwRnXk_yCPtnqqo4_u2YGL
https://www.youtube.com/watch?v=edIMMTL2jlw&list=PLVBorYCcu-xX3Ppjb_sqBd_Xf6GqagQyl
https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26
https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4
https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c
https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682
https://skymind.ai/wiki/generative-adversarial-network-gan
https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a
https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394
https://www.lexalytics.com/lexablog/machine-learning-vs-natural-language-processing-part-1
https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc
https://medium.com/cityai/deep-learning-for-natural-language-processing-part-i-8369895ffb98
https://machinelearningmastery.com/deep-learning-for-nlp/
https://towardsdatascience.com/using-nlp-and-deep-learning-to-predict-the-stock-market-64eb9229e102
https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/
https://www.cse.unsw.edu.au/~cs9417ml/RL1/index.html
https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/
https://medium.com/swlh/introduction-to-reinforcement-learning-coding-sarsa-part-4-2d64d6e37617
https://yanpanlau.github.io/2016/10/11/Torcs-Keras.html
https://medium.freecodecamp.org/how-to-build-an-ai-game-bot-using-openai-gym-and-universe-f2eb9bfbb40a
https://medium.com/@jonathan_hui/rl-basics-algorithms-and-terms-ae98314851d7
https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690
https://sandipanweb.wordpress.com/2017/03/24/solving-4-puzzles-with-reinforcement-learning-q-learning-in-python/
https://medium.com/@asierarranz/decentralized-supervised-neural-network-on-the-blockchain-giving-mining-a-good-purpose-3e64888caa
https://medium.com/beyond-intelligence/reinforcement-learning-or-evolutionary-strategies-nature-has-a-solution-both-8bc80db539b3
https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287
https://medium.freecodecamp.org/using-machine-learning-to-predict-the-quality-of-wines-9e2e13d7480d
https://blog.openai.com/openai-baselines-ppo/
https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa
https://towardsdatascience.com/build-your-own-convolution-neural-network-in-5-mins-4217c2cf964f
https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/
https://medium.com/@AdrienLE/lets-build-an-atari-ai-part-0-intro-to-rl-9b2c5336e0ec
https://towardsdatascience.com/using-nlp-and-deep-learning-to-predict-the-stock-market-64eb9229e102
https://machinelearningmastery.com/deep-learning-for-nlp/
https://medium.com/cityai/deep-learning-for-natural-language-processing-part-i-8369895ffb98
https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc
https://www.lexalytics.com/lexablog/machine-learning-vs-natural-language-processing-part-1
https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394
https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a
https://skymind.ai/wiki/generative-adversarial-network-gan
https://www.researchgate.net/publication/224089748_Malware_detection_using_machine_learning
https://github.com/Kaixhin/Rainbow
https://github.com/endgameinc/gym-malware
https://github.com/Kyushik/DRL 
https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4
https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26
https://becominghuman.ai/beat-atari-with-deep-reinforcement-learning-part-2-dqn-improvements-d3563f665a2c
https://www.blog.google/technology/ai/alphago-machine-learning-game-go/
https://www.nature.com/articles/nature16961
https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682
https://sigmoidal.io/boosting-your-solutions-with-nlp/
https://github.com/JannesKlaas/sometimes_deep_sometimes_learning/blob/master/reinforcement.ipynb
https://www.youtube.com/watch?v=3bhP7zulFfY
https://medium.freecodecamp.org/deep-reinforcement-learning-where-to-start-291fb0058c01
https://magenta.tensorflow.org/2016/11/09/tuning-recurrent-networks-with-reinforcement-learning
https://github.com/MattChanTK/gym-maze
https://github.com/ChintanTrivedi/DeepGamingAI_FIFARL
github.com/keon/deep-q-learning/blob/master/dqn.py
https://www.youtube.com/watch?v=FmpDIaiMIeA
https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164
https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762
https://medium.com/@yashpatel_86510/reinforcement-learning-w-keras-openai-698add10b4eb
https://github.com/keon/deep-q-learning
https://keon.io/deep-q-learning/
https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html
https://www.youtube.com/watch?v=A5eihauRQvo
https://www.youtube.com/watch?v=79pmNdyxEGo
https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3
https://www.youtube.com/watch?v=0Zuqytgf6yY
https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721
https://www.youtube.com/watch?v=pieI7rOXELI&list=PLXO45tsB95cIplu-fLMpUEEZTwrDNh6Ba
https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT
https://www.youtube.com/watch?v=wQ8BIBpya2k&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN
https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0
https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795
https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe
https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287
http://adventuresinmachinelearning.com/keras-lstm-tutorial/
https://medium.com/deep-learning-101/how-to-generate-a-video-of-a-neural-network-learning-in-python-62f5c520e85c
https://blog.coast.ai/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6
https://blog.coast.ai/reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-part-2-93e614fcd238
https://blog.coast.ai/reinforcement-learning-in-python-to-teach-an-rc-car-to-avoid-obstacles-part-3-a1d063ac962f
http://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/
http://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/
https://www.activestate.com/blog/2017/05/building-game-ai-using-machine-learning-working-tensorflow-keras-and-intel-mkl-python
https://www.pyimagesearch.com/2017/12/18/keras-deep-learning-raspberry-pi/
https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2
https://medium.com/deep-math-machine-learning-ai/ch-13-deep-reinforcement-learning-deep-q-learning-and-policy-gradients-towards-agi-a2a0b611617e
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0
https://github.com/yanpanlau/Keras-FlappyBird
https://ai.intel.com/demystifying-deep-reinforcement-learning/
https://github.com/yenchenlin/DeepLearningFlappyBird
https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8
https://youtube.com/watch?v=Aut32pR5PQA => genetic algorithm
https://stackoverflow.com/questions/44747343/keras-input-explanation-input-shape-units-batch-size-dim-etc
https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f
https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75
https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8
https://www.youtube.com/watch?v=JcI5Vnw0b2c&index=2&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v
https://www.youtube.com/watch?v=wQ8BIBpya2k
https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=1
https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6
https://www.quora.com/How-do-artificial-neural-networks-work
https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419
https://enlight.nyc/projects/neural-network/
https://appdividend.com/2018/07/23/prepare-dataset-for-machine-learning-in-python/
https://www.github.com/SpiderLabs/social_mapper => read its code to build smt like social_mapper with AI but more powerfull than that
https://www.theverge.com/2018/8/8/17663640/socialmapper-facial-recognition-open-source-intelligence
https://thehackernews.com/2018/08/artificial-intelligence-malware.html
https://thehackernews.com/2018/08/social-mapper-osint.html
https://github.com/tensorflow/cleverhans
https://www.youtube.com/watch?v=fm8MGPKgUPk
https://www.youtube.com/watch?v=_U146hWhDhM
https://www.youtube.com/watch?v=m2w_FCba8TY&list=PLnNF__MXrUQy7Stv8sc4U7pyjf7oNk0tl
https://www.youtube.com/watch?v=XZkiuWOf8TI&list=PLVRZqxcZ5vU9J0Wh4Lsh3mGguZuJm7hKw
https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A/playlists
https://www.youtube.com/watch?v=b81Ib_oYbFk
https://www.youtube.com/watch?v=F0njE7D22SI
https://stackoverflow.com/questions/44429742/need-reference-of-opencv-python-for-age-and-gender-detection-on-video-stream
https://github.com/BoyuanJiang/Age-Gender-Estimate-TF
https://github.com/dpressel/rude-carnie
https://gizmodo.com/hackers-have-already-started-to-weaponize-artificial-in-1797688425
https://aitrends.com/ai-insider/ai-deep-learning-backdoor-security-holes-self-driving-cars-detection-prevention/
https://www.theregister.co.uk/2017/12/20/fool_ai_facial_recognition_poison/
https://www.forbes.com/sites/ajagrawal/2017/08/01/how-to-use-artificial-intelligence-to-growth-hack-social-engagement/#4cb592f318c9
https://resources.infosecinstitute.com/criminals-can-exploit-ai/
https://www.analyticsvidhya.com/blog/2017/02/6-deep-learning-applications-beginner-python/
https://www.oscaralsing.com/automated-tinder-using-artificial-intelligence/
https://hackernoon.com/what-leading-artificial-intelligence-course-should-you-take-and-what-should-you-do-after-261a933bb3da
https://motherboard.vice.com/en_us/article/ev8gx4/were-teaming-up-with-the-plug-a-daily-newsletter-on-black-entrepreneurs-in-tech
http://letzgro.net/blog/creating-ai-using-python/
https://aibusiness.com/ai-researchers-develop-backdoor-security-threat/
https://ieeexplore.ieee.org/document/6086325/
https://www.wired.com/story/machine-learning-backdoors/
https://www.bleepingcomputer.com/news/security/ai-training-algorithms-susceptible-to-backdoors-manipulation/
https://qz.com/1061560/researchers-built-an-invisible-back-door-to-hack-ais-decisions/
https://contest-2017.korelogic.com/
https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6
https://medium.com/@curiousily/tensorflow-for-hackers-part-i-basics-2c46bc99c930
https://medium.com/mindorks/detection-on-android-using-tensorflow-a3f6fe423349
https://hackernoon.com/finding-the-genre-of-a-song-with-deep-learning-da8f59a61194
https://mentalhealthdaily.com/2014/04/15/5-types-of-brain-waves-frequencies-gamma-beta-alpha-theta-delta/
https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491
https://cointelegraph.com/news/blockchain-based-artificial-neural-networks-to-save-thousands-of-lives-from-medical-errors
https://www.youtube.com/results?search_query=ANN+exploiting+in+python
https://www.youtube.com/watch?v=gWfVwnOyG78&list=PLnjEM1fs09cE7SAFuvU1URVgbBgXZnNmF
https://www.youtube.com/watch?v=hENZrj0qxqg&list=PLnjEM1fs09cGQvwi-HQGB9hz-v-cWJBv3
https://www.youtube.com/watch?v=oxf3o8IbCk4
https://www.youtube.com/watch?v=gplTc2F5Wvk
https://www.quora.com/Is-it-possible-to-access-the-Internet-without-an-ISP?share=1
https://www.quora.com/How-can-you-access-the-internet-without-an-ISP?share=1
http://www.abovetopsecret.com/forum/thread1021068/pg1
https://www.youtube.com/watch?v=PWLffkqw_yY
https://medium.com/ethereum-dapp-builder/blockchain-and-how-to-create-a-dapp-daa1a548ff51
https://hackernoon.com/7-blockchain-web-development-tools-that-will-grow-your-stack-3d854f1cb9bf
https://medium.com/@reputaction/decentralized-application-dapp-platform-selection-c7790ea74bce
'''

'''
-example of iot and dapp using blockchain solidity cop and open source kits:
	IBM blockchain, telegram and blockchain based storage, youtube, books in WOShelf folder, pet-shop tutor(testrpc for local developement), ethereum blockchain, public and private blockchain
	pet-shop tutor source: http://truffleframework.com/tutorials/pet-shop
	scripting languages(truffle and ethereum stack => the BlockchainTech folder and CRYPTOGRAPHY_CRYPTANALYSIS section), use IPFS and python => decentralized and distributed storage
-example of iot, fraud detection system of a bank and shopstore or movie org using DB graph neo4j and big data_data mining concept and algorithms:
    youtube, books in WOShelf folder, neo4j browser or its app & scripting languages(nodejs and python), use IPFS and python => decentralized and distributed storage(BlockchainTech folder)
-build your cv using your ideas and their related technology by using p2p technology and its concept, algorithm and iot stuffs & devices
-my ideas using python in everything section:
    proof-of-Dreaming_Flow-idea|Build A Predictable Future Machine + its Dapp with ethereum stack:
    	programming languages: python +â€Œ ethereum stack
        CONCEPTS: simulation, iot concept, probabilistic 
        TODO: use presentation tools in my phone to control my conferences using the phone + also use airdroid to control the phone with my lap top on LAN
        TODO: find a good app to create my vocfu style slides for all about Machine/deep Learning(ANN) vs data mining in AI and my idearuning
        TOKNOW: understand the graphical computational model of Tensorflow and its ANN
        TOKNOW: data mining and big data concepts and their algorithms + matrices and graph concepts to know the entire algorithm + oneirology
        references: http://web.stanford.edu/class/cs20si/lectures/, http://cs224d.stanford.edu/lectures/, http://tensorflowbook.com/, https://www.youtube.com/watch?v=2Nih24Hy5ng
        references: http://www.tutdl.ir/blog/1396/04/25/%d8%af%d8%a7%d9%86%d9%84%d9%88%d8%af-%d8%a2%d9%85%d9%88%d8%b2%d8%b4-data-science-deep-learning-in-python/, 
        references: http://www.tutdl.ir/blog/1396/05/13/%d8%af%d8%a7%d9%86%d9%84%d9%88%d8%af-%d8%a2%d9%85%d9%88%d8%b2%d8%b4-python-for-data-science-and-machine-learning-bootcamp/, 
        references: http://www.tutdl.ir/blog/1396/08/12/%d8%af%d8%a7%d9%86%d9%84%d9%88%d8%af-%d8%a2%d9%85%d9%88%d8%b2%d8%b4-complete-guide-to-tensorflow-for-deep-learning-with-python/, 
        references: http://www.tutdl.ir/blog/1396/09/17/%d8%af%d8%a7%d9%86%d9%84%d9%88%d8%af-%d8%a2%d9%85%d9%88%d8%b2%d8%b4-the-complete-machine-learning-course-with-python/,
        references: https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_tutorials.html, https://www.pyimagesearch.com/2017/11/27/image-hashing-opencv-python/ 
        references: https://www.kdnuggets.com/2015/05/top-10-data-mining-algorithms-explained.html, https://www.springboard.com/blog/data-mining-python-tutorial/
        references: https://www.youtube.com/watch?v=DJjPzyo3osg, https://www.youtube.com/watch?v=XYk4Xtad0Bg, https://www.youtube.com/watch?v=AYhHJIAbNIg
        references: WOShelf books and TensorFlow For Machine Intelligence book, https://www.youtube.com/watch?v=BSpAWkQLlgM, https://www.youtube.com/watch?v=w1xNTLH1zlA
        tools: open source kits and hardwares(Raspberry Pi) to fire up the machine or maybe we want to use quantum computing machine
        tools: blockchain truffle COP to create contracts between dreams using their public address key 
        tools: ANN tensorFlow(python) for its machine(deep) learning in AI section to create images of objects and events in dream using recorded brain waves and reverse engineering 
        tools: graph DB neo4j(nodejs/python) to store all objects in every dream or we can use IPFS or storj => decentralized and distributed storage(BlockchainTech folder)
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    proof-of-Brain's_Password_Cracker-idea => Build A Device Like Specific Neurons Using ANN To Detect The Words & Phrases Or Read Whole Things Of Mind And Save Them In A Safe TXT File Or We Can Send Them By Wifi Or A New Protocol In A Second When User Is Typing The Credentials; User's Brain Must Know This Device As Our Arbitary(Specific) Neuron To Give The Data To Our
    Device Using Brain's And Device's Synapses, Maybe We Have To Use Quantum Computing Machine; we can use IPFS or storj => decentralized and distributed storage(BlockchainTech folder)
    	CONCEPTS: simulation, iot concept, probabilistic
    	programming languages: python + ethereum stack
	AI-TYPE : Artificial Super Intelligence
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    proof-of-Telegram_Sloving_Question_Bot-idea => Build A Telegram Bot To Solve An Specific Question From Its Picture And Send Us The Answer In A picture
    xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
    proof-of-Doublegainer-idea => Build A Virtual Brain From The Real One Which Is The Human Brain In Order To Do All Remaining Stuffs And Works Of Human Life That Is related To Another BlockchainTechs + its Dapp with ethereum stack:
        CONCEPTS: simulation, iot concept, probabilistic
        TO NEED: we need a blockchain based storage like sia for our contracts between the human brain and the virtual one to store rapidly the data and transactions in blockchain
        programming languages: python + ethereum stack
        references: https://blog.sia.tech/how-to-put-data-on-the-sia-network-784499a65b, http://simpy.readthedocs.io/en/latest/simpy_intro/, https://pypi.python.org/pypi/simpy
        TODO: we can use IPFS or storj => decentralized and distributed storage(BlockchainTech folder)
        TODO: build a device to act like neuron to get the data from real brain using its synapses and send them to the virtual brain
        TODO: virtual brain must check the ownership of neuron device using its UUID or a unique behaviour address
        TODO: the virtual brain collect data into different categories like emotion works; logical works; anger works and so on using ANN or AI
        TODO: then virtual brain stores all categories which every category has its own works in our blockchain and all works have a public address in order to interact with each others and 		other categories using smart contracts with truffle and COP concept
        TODO: create your neural networks
        TODO: add citizen ship idea(AI blockchain based using smart contract with validithum as its crypto crurrency money to create a smart citizen and change the constitution)
        TODO: for its proof of work we have some systems for our users which are the nodes in our entire network and check two rules for the incoming data from our neuron device:
              check the ownership of virtual brain so every real brain must hav UUID(or a unique address of our neuron device stuck(later on we can use brain waves to detect the thoughts of real brain and send those waves to our virtual brain without using a neuron device) on users brain)
              either is a new or old data if is new it will added to the blockchain with its category and real brain info like its address and its waves; if is old the virtual brain must detect the rest of work or what was the final result of that using ANN or AI
        PURPOSE: we want to build another human not a robot ofcourse but a virtual brain which it will do all human works and make all human decisions according to the order of human real
        	     brain; the human brain must active the virtual mode with a kind of signal or some orders or some thoughts(brain waves thoughts)
        PURPOSE: this technology is usefull for interacting with other blockchain and distributed systems to have copy of a human brain and make presence of human less in computer world
        		 we can use it in mental payment system, book writing & create movie or make a new language from user's mind using ANN or AI concept to show the right concept of what he/she is thinking or imagining to the other people or other blockchain system finally write your about-me book!
'''
# -------------------------------------------------------------------------------------------------------------------------------------------
# import random
# import gym
# import numpy as np
# from collections import deque
# from keras.models import Sequential
# from keras.layers import Dense, Activation, Flatten
# from keras.optimizers import Adam
# import matplotlib.pyplot as plt
# from keras import backend as K
# import tensorflow as tf
# from rl.agents.dqn import DQNAgent
# from rl.policy import BoltzmannQPolicy
# from rl.memory import SequentialMemory

# EPISODES = 1000 # a number of games we want the agent to play.

# class DQNAgent:
#     def __init__(self, state_size, action_size):
#         self.state_size = state_size
#         self.action_size = action_size
#         # print(self.action_size, self.state_size)
#         self.memory = deque(maxlen=2000)
#         self.gamma = 0.95    # discount rate - aka decay or discount rate, to calculate the future discounted reward.
#         self.epsilon = 1.0  # aka exploration rate, this is the rate in which an agent randomly decides its action rather than prediction.
#         self.epsilon_min = 0.01 # we want the agent to explore at least this amount.
#         self.epsilon_decay = 0.995 # we want to decrease the number of explorations as it gets good at playing games.
#         self.learning_rate = 0.001 # Determines how much neural net learns in each iteration.
#         self.model = self._build_model()
#         self.target_model = self._build_model()
#         self.update_target_model()


#     def _huber_loss(self, y_true, y_pred, clip_delta=1.0):
#         error = y_true - y_pred
#         cond  = K.abs(error) <= clip_delta

#         squared_loss = 0.5 * K.square(error)
#         quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)

#         return K.mean(tf.where(cond, squared_loss, quadratic_loss))

#     def _build_model(self):
#         # Neural Net for Deep-Q learning Model
#         model = Sequential()
#         # 'Dense' is the basic form of a neural network layer
#         # Input Layer of state size(4) and Hidden Layer with 24 nodes
#         model.add(Dense(24, input_dim=self.state_size, activation='relu'))
#         # Hidden layer with 24 nodes
#         model.add(Dense(24, activation='relu'))
#         # Output Layer with # of actions: 2 nodes (left, right)
#         model.add(Dense(self.action_size, activation='linear'))
#         model.compile(loss=self._huber_loss,
#                       optimizer=Adam(lr=self.learning_rate))
#         return model
    
#     def update_target_model(self):
#         """copy weights from model to target_model ; 
#         cause the weights will update in every epoch we won't get the our target 
#         and cause of that we're using another brain here"""
#         self.target_model.set_weights(self.model.get_weights()) # only replace the weights of our new model with the existing model

#     def remember(self, state, action, reward, next_state, done):
#         """ One of the challenges for DQN is that neural network used in the algorithm tends to forget the previous experiences 
#             as it overwrites them with new experiences. So we need a list of previous experiences and observations to re-train 
#             the model with the previous experiences. We will call this array of experiences memory and use remember() function to append state, action, reward, and next state to the memory.
#         """
        
#         self.memory.append((state, action, reward, next_state, done))

#     def act(self, state):
#         """ Our agent will randomly select its action at first by a certain percentage, called â€˜exploration rateâ€™ or â€˜epsilonâ€™. 
#             This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. 
#             When it is not deciding the action randomly, the agent will predict the reward value based on the current state and 
#             pick the action that will give the highest reward. np.argmax() is the function that picks the highest value between two elements in the act_values[0].
#         """
        
#         if np.random.rand() <= self.epsilon:
#             return random.randrange(self.action_size) # choose a random number between possible actions(left or right) to reach a state
#         act_values = self.model.predict(state) 
#         # print(act_values)
#         # act_values =:
#         # [[0.2342021  0.33798292]]
#         # [[0.20367415 0.3270337 ]]
#         # .....
#         # [[0.28844938 0.33308575]]
#         # returns the maximum action between existing actions(left and right) which calculated by the NN
#         # (eg, 0.67 go to right and 0.33 go to left then it'll choose the right direction for our state vector)
#         return np.argmax(act_values[0])

#     def replay(self, batch_size):
#         """ A method that trains the neural net with experiences in the memory is called replay(). First, we sample some experiences from the memory and call them minibath.
#             minibatch = random.sample(self.memory, batch_size)
#             The above code will make minibatch, which is just a randomly sampled elements of the memories of size batch_size. We set the batch size as 32 for this example.
#             To make the agent perform well in long-term, we need to take into account not only the immediate rewards but also the future rewards we are going to get. 
#             In order to do this, we are going to have a â€˜discount rateâ€™ or â€˜gammaâ€™. This way the agent will learn to maximize the discounted future reward based on the given state 
#             However, the problem is that we using the same parameters (weights) for estimating the target and the Q value. As a consequence, there is a big correlation between the TD target and the parameters (w) we are changing.
#             Therefore, it means that at every step of training, our Q values shift but also the target value shifts. So, weâ€™re getting closer to our target but the target is also moving. Itâ€™s like chasing a moving target! This lead to a big oscillation in training.
#             By calculating the TD target, we face a simple problem: how are we sure that the best action for the next state is the action with the highest Q-value?
#             We know that the accuracy of q values depends on what action we tried and what neighboring states we explored.
#             As a consequence, at the beginning of the training we donâ€™t have enough information about the best action to take. Therefore, taking the maximum q value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly given a higher Q value than the optimal best action, the learning will be complicated.
#             The solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q value generation. We:
#             use our DQN network to select what is the best action to take for the next state (the action with the highest Q value).
#             use our target network to calculate the target Q value of taking that action at the next state.
#         """
        
#         # print(self.memory)
#         minibatch = random.sample(self.memory, batch_size)
#         # minibactch =: deque([(array([[-0.07520104, -0.37955881,  0.06440256,  0.54503593]]), 1, 1.0, array([[-0.08279222, -0.18539812,  0.07530328,  0.27331995]]), False), 
#         #                      (array([[ 0.00108801,  0.00934971, -0.04501215, -0.01184072]]), 0, 1.0, array([[ 0.00127501, -0.18509877, -0.04524897,  0.26630747]]), False), 
#         #                      (array([[-0.08632872,  0.20244938,  0.0808758 , -0.26083994]]), 1, 1.0, array([[-0.08227973,  0.39632928,  0.075659  , -0.52695763]]), False),
#         #                      (array([[-0.00380544,  1.36878757, -0.03151045, -1.91940555]]), 1, 1.0, array([[ 0.02357031,  1.56423369, -0.06989856, -2.22169174]]), False),  
#         #                       ...., 
#         #                      (array([[ 0.08225186,  1.56598078, -0.15335945, -2.27717757]]), 1, 1.0, array([[ 0.11357148,  1.76216002, -0.198903  , -2.61290389]]), False) ])
#         # states, targets_f = [], []
#         for state, action, reward, next_state, done in minibatch:
#             # eg: reward =: 1.0 , state =: [[-0.02663719 -0.2388644  -0.01880481  0.2460182 ]], maximum integer action calculated from NN =: 1
#             # target = reward # Q(s,a) = R
#             target = self.model.predict(state)
#             if done:
#                 # The loss is just a value that indicates how far our prediction is from the actual target. For example, 
#                 # the prediction of the model could indicate that it sees more value in pushing the right button when in fact 
#                 # it can gain more reward by pushing the left button. We want to decrease this gap between the prediction and the target (loss). We will define our loss function as follows:
#                 # We first carry out an action a, and observe the reward r and resulting new state s`. Based on the result, 
#                 # we calculate the maximum target Q and then discount it so that the future reward is worth less than immediate reward 
#                 # (It is a same concept as interest rate for money. Immediate payment always worth more for same amount of money). 
#                 # Lastly, we add the current reward to the discounted future reward to get the target value. Subtracting our current 
#                 # prediction from the target gives the loss. Squaring this value allows us to punish the large loss value more and treat the negative values same as the positive values.
#                 # Keras takes care of the most of the difficult tasks for us. We just need to define our target. We can express the target in a magical one-liner in python.
#                 # self.model.predict(next_state)[0] => calculated action from NN 
#                 # target = (reward + self.gamma *
#                 #           np.amax(self.model.predict(next_state)[0])) # returns the maximum action between all existing actions which calculated by the NN
#                 target[0][action] = reward
#             else:
#                 t = self.target_model.predict(next_state)[0] # predict the value for Q(Sâ€™,Aâ€™) in which the output are left or right and we're gonna get the maximum action between predicted actions 
#                 # we can achieve optimal policies for our games by estimating the Q(s, a) function, 
#                 # which gives us an estimate of the discounted sum of rewards of taking action a in state s, 
#                 # and playing optimally thereafter. Playing the action with the maximum Q-value in any given state is the same as playing optimally!
#                 # The question is now: how do we estimate Q(s, a)? Wellâ€¦ how do you estimate any function these days? 
#                 # With a deep neural network of course! Or as you might call it, a Deep Q-Network (DQN).
#                 target[0][action] = reward + self.gamma * np.amax(t) # Q function algo
#             # we are losing our weights in every epoch in training samples so cuase of this 
#             # we are not able to reach the target value cause the target is also moving and
#             # (we calculate the traget value with our first brain also) the solution is use another brain to copy 
#             # our first brain synapses(weights) on target brains and calculate its value
#             # and then we can train our samples in our first brain in 20 epochs so by this job 
#             # we don't have to be worry about losing our weights and get far away from our target
#             # cause we calculated our target value with another brain and the epoch is happening in another brain!
#             """ML problems start with dataâ€”preferably, lots of data (examples or observations) 
#             for which you already know the target answer. Data for which you already know 
#             the target answer is called labeled data. ... You provide data that is labeled 
#             with the target (correct answer) to the ML algorithm to learn from."""
#             self.model.fit(state, target, epochs=20, verbose=0) # train the model on 20 epochs to close itself to the target
#             # print(target)
#             # After training, the model now can predict the output from unseen input. When you call predict() function on the model, 
#             # the model will predict the reward of current state by giving the action based on the data you trained
#             # target_f = self.model.predict(state) # prediction
#             # # target_f =: 
#             # # [[ 0.21216568 -0.00235922]]
#             # # [[0.34335896 0.03951475]]
#             # # [[0.04028061 0.02595955]]
#             # # [[0.07161053 0.00580003]]
#             # # .
#             # # .
#             # # .
#             # # [[ 0.13222136 -0.03803551]]
#             # # [[ 0.13657948 -0.04835601]]
#             # # [[ 0.13386746 -0.04498314]]
#             # target_f[0][action] = target 
#             # # Filtering out states and targets for training
#             # states.append(state[0])
#             # targets_f.append(target_f[0])
#             # print(target_f[0]) # eg: [ 1.162856   -0.01493281]
#             # print(state[0]) # eg: [ 0.06730458 -0.00458529  0.02341822  0.1553169 ]
#             # fit() method feeds input and output pairs to the model. Then the model will train on those data to approximate the output based on the input.
#             # This training process makes the neural net to predict the reward value from a certain state.
#             # epoch is a measure of the number of times all of the training vectors are used 
#             # once to update the weights. For batch training all of the training samples pass 
#             # through the learning algorithm simultaneously in one epoch before weights are updated.
#         # model.fit(x=None, y=None, ...) => Trains the model for a given number of epochs (iterations on a dataset).
#         # x: Numpy array of training data 
#         # y: Numpy array of target (label) data
#         # print(states, " ====== ", targets_f)
#         # =======EPOCH=======
#         # An epoch is a single step in training a neural network; 
#         # in other words when a neural network is trained on every 
#         # training samples only in one pass we say that one epoch 
#         # is finished. So training process may consist more than one epochs.
#         # history = self.model.fit(np.array(states), np.array(targets_f), epochs=5, verbose=0)
#         # Keeping track of loss
#         # loss = history.history['loss'][0]
#         # print(loss)
#         if self.epsilon > self.epsilon_min:
#             self.epsilon *= self.epsilon_decay
#         # return loss

#     def load(self, name):
#         self.model.load_weights(name)

#     def save(self, name):
#         self.model.save_weights(name)


# if __name__ == "__main__":
#     env = gym.make('CartPole-v1')
#     state_size = env.observation_space.shape[0]
#     # print(state_size) # 4 =: left , right , balance , slide or drop the pole
#     action_size = env.action_space.n
#     # print(action_size) # 2
#     agent = DQNAgent(state_size, action_size)
#     smodel = agent._build_model()
#     smodel.summary()
#     # agent.load("cartpole-dqn.h5")
#     done = False
#     batch_size = 32

#     for e in range(EPISODES):
#         # reset state in the beginning of each game
#         state = env.reset()
#         state = np.reshape(state, [1, state_size]) # turn the state into a one dimensional matrix which is a vector
#         # time represents each frame of the game
#         # Our goal is to keep the pole upright as long as possible until score of 500
#         # the more time the more score
#         for time in range(700):
#             env.render()
#             # Decide action
#             action = agent.act(state) # maximum action ; pass our vector state to our NN in which we have state_size neurons
#             # Advance the game to the next frame based on the action.
#             # Reward is 1 for every frame the pole survived
#             next_state, reward, done, _ = env.step(action)
#             reward = reward if not done else -10
#             # we are turning our next_state into a one dimensional matrix which is a vector
#             # to calculate the maximum future reward for next state ; cause our model input 
#             # is a one dimensional matrix which is a vector in which in our case is 4 neurons
#             next_state = np.reshape(next_state, [1, state_size])
#             # Remember the previous state, action, reward, and done
#             agent.remember(state, action, reward, next_state, done)
#             # make next_state the new current state for the next frame.
#             state = next_state
#             # done becomes True when the game ends
#             # ex) The agent drops the pole
#             if done:
#                 agent.update_target_model()
#                 print("episode: {}/{}, score: {}, e: {:.2}"
#                       .format(e, EPISODES, time, agent.epsilon))
#                 break
#             if len(agent.memory) > batch_size:
#                 # train the agent with the experience of the episode
#                 # loss = agent.replay(batch_size)
#                 agent.replay(batch_size)
#                 # Logging training loss every 10 timesteps
#                 # if time % 10 == 0:
#                 #     print("episode: {}/{}, time: {}, loss: {:.4f}"
#                 #         .format(e, EPISODES, time, loss))  
# #         if e % 10 == 0:
# #             agent.save("cartpole-dqn.h5")

# # --------------------------------------------------------------------------------------------------------------------------------------------------------

# # SARSA Q-TABLE UPDATE EQUATIION
# # Q(s,a) = Q(s,a) + alpha(R + gamma*Q(s`,a`) - Q(s,a)) => regardless of negetive and positive value we'll use mse as our loss function

# # KERAS-RL

# ENV_NAME = 'CartPole-v0'

# # Get the environment and extract the number of actions.
# env = gym.make(ENV_NAME)
# np.random.seed(123)
# env.seed(123)
# nb_actions = env.action_space.n

# # Next, we build a very simple model regardless of the dueling architecture
# # if you enable dueling network in DQN , DQN will build a dueling network base on your model automatically
# # Also, you can build a dueling network by yourself and turn off the dueling network in DQN.
# model = Sequential()
# model.add(Flatten(input_shape=(1,) + env.observation_space.shape))
# model.add(Dense(16))
# model.add(Activation('relu'))
# model.add(Dense(16))
# model.add(Activation('relu'))
# model.add(Dense(16))
# model.add(Activation('relu'))
# model.add(Dense(nb_actions, activation='linear'))
# print(model.summary())

# # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and
# # even the metrics!
# memory = SequentialMemory(limit=50000, window_length=1)
# policy = BoltzmannQPolicy()
# # enable the dueling network
# # you can specify the dueling_type to one of {'avg','max','naive'}
# dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
#                enable_dueling_network=True, dueling_type='avg', target_model_update=1e-2, policy=policy)
# dqn.compile(Adam(lr=1e-3), metrics=['mae'])

# # Okay, now it's time to learn something! We visualize the training here for show, but this
# # slows down training quite a lot. You can always safely abort the training prematurely using
# # Ctrl + C.
# dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)

# # After training is done, we save the final weights.
# dqn.save_weights('duel_dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)

# # Finally, evaluate our algorithm for 5 episodes.
# dqn.test(env, nb_episodes=5, visualize=False)

# --------------------------------------------------------------------------------------------------------------------------------------------------------
"""
Q-learning intuition
A good way to understand Q-learning is to compare playing Catch with playing chess.
In both games you are given a state, S. With chess, this is the positions of the figures on the board. In Catch, this is the location of the fruit and the basket.
The player then has to take an action, A. In chess, this is moving a figure. In Catch, this is to move the basket left or right, or remain in the current position.
As a result, there will be some reward R, and a new state Sâ€™.
The problem with both Catch and chess is that the rewards do not appear immediately after the action.
In Catch, you only earn rewards when the fruits hit the basket or fall on the floor, and in chess you only earn a reward when you win or lose the game. This means that rewards are sparsely distributed. Most of the time, R will be zero.
When there is a reward, it is not always a result of the action taken immediately before. Some action taken long before might have caused the victory. Figuring out which action is responsible for the reward is often referred to as the credit assignment problem.
Because rewards are delayed, good chess players do not choose their plays only by the immediate reward. Instead, they choose by the expected future reward.
For example, they do not only think about whether they can eliminate an opponentâ€™s figure in the next move. They also consider how taking a certain action now will help them in the long run.
In Q-learning, we choose our action based on the highest expected future reward. We use a â€œQ-functionâ€ to calculate this. This is a math function that takes two arguments: the current state of the game, and a given action.
We can write this as: Q(state, action)
While in state S, we estimate the future reward for each possible action A. We assume that after we have taken action A and moved to the next state Sâ€™, everything works out perfectly.
The expected future reward Q(S,A) for a given a state S and action A is calculated as the immediate reward R, plus the expected future reward thereafter Q(S',A'). We assume the next action A' is optimal.
Because there is uncertainty about the future, we discount Q(Sâ€™,Aâ€™) by the factor gamma Î³.
Q(S,A) = R + Î³ * max Q(Sâ€™,Aâ€™)
Good chess players are very good at estimating future rewards in their head. In other words, their Q-function Q(S,A) is very precise.
Most chess practice revolves around developing a better Q-function. Players peruse many old games to learn how specific moves played out in the past, and how likely a given action is to lead to victory.
But how can a machine estimate a good Q-function? This is where neural networks come into play.
Regression after all
When playing a game, we generate lots of â€œexperiencesâ€. These experiences consist of:
the initial state, S
the action taken, A
the reward earned, R
and the state that followed, Sâ€™
These experiences are our training data. We can frame the problem of estimating Q(S,A) as a regression problem. To solve this, we can use a neural network.
Given an input vector consisting of S and A, the neural net is supposed to predict the value of Q(S,A) equal to the target: R + Î³ * max Q(Sâ€™,Aâ€™).
If we are good at predicting Q(S,A) for different states S and actions A, we have a good approximation of the Q-function. Note that we estimate Q(Sâ€™,Aâ€™) through the same neural net as Q(S,A).
The training process
Given a batch of experiences < S, A, R, Sâ€™ >, the training process then looks as follows:
For each possible action Aâ€™ (left, right, stay), predict the expected future reward Q(Sâ€™,Aâ€™) using the neural net
Choose the highest value of the three predictions as max Q(Sâ€™,Aâ€™)
Calculate r + Î³ * max Q(Sâ€™,Aâ€™). This is the target value for the neural net
Train the neural net using a loss function. This is a function that calculates how near or far the predicted value is from the target value. Here, we will use 0.5 * (predicted_Q(S,A)â€Šâ€”â€Štarget)Â² as the loss function.
During gameplay, all the experiences are stored in a replay memory. This acts like a simple buffer in which we store < S, A, R, Sâ€™ > pairs. The experience replay class also handles preparing the data for training. 
"""


"""
Here we define some variables used for the game and rendering later
"""
# last frame time keeps track of which frame we are at
# last_frame_time = 0
# #translate the actions to human readable words
# translate_action = ["Left","Stay","Right","Create Ball","End Test"]
# #size of the game field
# grid_size = 10
# # parameters
# epsilon = .1  # exploration
# num_actions = 3  # [move_left, stay, move_right]
# max_memory = 500 # Maximum number of experiences we are storing
# hidden_size = 100 # Size of the hidden layers
# batch_size = 1 # Number of experiences we use for training per batch

# def display_screen(action,points,input_t):
#     #Function used to render the game screen
#     #Get the last rendered frame
#     global last_frame_time
#     print("Action %s, Points: %d" % (translate_action[action],points))
#     #Only display the game screen if the game is not over
#     if("End" not in translate_action[action]):
#         #Render the game with matplotlib
#         plt.imshow(input_t.reshape((grid_size,)*2),
#                interpolation='none', cmap='gray')
#         #Clear whatever we rendered before
#         # display.clear_output(wait=True)
#         #And display the rendering
#         plt.gcf()
#         # display.display(plt.gcf())
#     #Update the last frame time
#     last_frame_time = set_max_fps(last_frame_time)
    
    
# def set_max_fps(last_frame_time,FPS = 1):
#     current_milli_time = lambda: int(round(time.time() * 1000))
#     sleep_time = 1./FPS - (current_milli_time() - last_frame_time)
#     if sleep_time > 0:
#         time.sleep(sleep_time)
#     return current_milli_time()


# def baseline_model(grid_size,num_actions,hidden_size):
#     #seting up the model with keras
#     model = Sequential()
#     model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))
#     model.add(Dense(hidden_size, activation='relu'))
#     model.add(Dense(num_actions))
#     model.compile(sgd(lr=.1), "mse")
#     return model

# def test(model):
#     #This function lets a pretrained model play the game to evaluate how well it is doing
#     global last_frame_time
#     plt.ion()
#     # Define environment, game
#     env = Catch(grid_size)
#     #c is a simple counter variable keeping track of how much we train
#     c = 0
#     #Reset the last frame time (we are starting from 0)
#     last_frame_time = 0
#     #Reset score
#     points = 0
#     #For training we are playing the game 10 times
#     for e in range(10):
#         loss = 0.
#         #Reset the game
#         env.reset()
#         #The game is not over
#         game_over = False
#         # get initial input
#         input_t = env.observe()
#         #display_screen(3,points,input_t)
#         c += 1
#         while not game_over:
#             #The learner is acting on the last observed game screen
#             #input_t is a vector containing representing the game screen
#             input_tm1 = input_t
#             #Feed the learner the current status and get the expected rewards for different actions from it
#             q = model.predict(input_tm1)
#             #Select the action with the highest expected reward
#             action = np.argmax(q[0])
#             # apply action, get rewards and new state
#             input_t, reward, game_over = env.act(action)
#             #Update our score
#             points += reward
#             display_screen(action,points,input_t)
#             c += 1

# def moving_average_diff(a, n=100):
#     diff = np.diff(a)
#     ret = np.cumsum(diff, dtype=float)
#     ret[n:] = ret[n:] - ret[:-n]
#     return ret[n - 1:] / n

# # GAME ENV
# class Catch():
#     """
#     Class catch is the actual game.
#     In the game, fruits, represented by white tiles, fall from the top.
#     The goal is to catch the fruits with a basket (represented by white tiles, this is deep learning, not game design).
#     """
#     def __init__(self, grid_size=10):
#         self.grid_size = grid_size
#         self.reset()

#     def _update_state(self, action):
#         """
#         Input: action and states
#         Ouput: new states and reward
#         """
#         state = self.state
#         if action == 0:  # left
#             action = -1
#         elif action == 1:  # stay
#             action = 0
#         else:
#             action = 1  # right
#         f0, f1, basket = state[0]
#         new_basket = min(max(1, basket + action), self.grid_size-1)
#         f0 += 1
#         out = np.asarray([f0, f1, new_basket])
#         out = out[np.newaxis]

#         assert len(out.shape) == 2
#         self.state = out

#     def _draw_state(self):
#         im_size = (self.grid_size,)*2
#         state = self.state[0]
#         canvas = np.zeros(im_size)
#         canvas[state[0], state[1]] = 1  # draw fruit
#         canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket
#         return canvas
        
#     def _get_reward(self):
#         fruit_row, fruit_col, basket = self.state[0]
#         if fruit_row == self.grid_size-1:
#             if abs(fruit_col - basket) <= 1:
#                 return 1
#             else:
#                 return -1
#         else:
#             return 0

#     def _is_over(self):
#         if self.state[0, 0] == self.grid_size-1:
#             return True
#         else:
#             return False

#     def observe(self):
#         canvas = self._draw_state()
#         return canvas.reshape((1, -1))

#     def act(self, action):
#         self._update_state(action)
#         reward = self._get_reward()
#         game_over = self._is_over()
#         return self.observe(), reward, game_over

#     def reset(self):
#         n = np.random.randint(0, self.grid_size-1, size=1)
#         m = np.random.randint(1, self.grid_size-2, size=1)
#         self.state = np.asarray([0, n, m])[np.newaxis]


# class ExperienceReplay():
#     """
#     During gameplay all the experiences < s, a, r, sâ€™ > are stored in a replay memory. 
#     In training, batches of randomly drawn experiences are used to generate the input and target for training.
#     """
#     def __init__(self, max_memory=100, discount=.9):
#         """
#         Setup
#         max_memory: the maximum number of experiences we want to store
#         memory: a list of experiences
#         discount: the discount factor for future experience
        
#         In the memory the information whether the game ended at the state is stored seperately in a nested array
#         [...
#         [experience, game_over]
#         [experience, game_over]
#         ...]
#         """
#         self.max_memory = max_memory
#         self.memory = list()
#         self.discount = discount

#     def remember(self, states, game_over):
#         #Save a state to memory
#         self.memory.append([states, game_over])
#         #We don't want to store infinite memories, so if we have too many, we just delete the oldest one
#         if len(self.memory) > self.max_memory:
#             del self.memory[0]

#     def get_batch(self, model, batch_size=10):
        
#         #How many experiences do we have?
#         len_memory = len(self.memory)
        
#         #Calculate the number of actions that can possibly be taken in the game
#         num_actions = model.output_shape[-1]
        
#         #Dimensions of the game field
#         env_dim = self.memory[0][0][0].shape[1]
        
#         #We want to return an input and target vector with inputs from an observed state...
#         inputs = np.zeros((min(len_memory, batch_size), env_dim))
        
#         #...and the target r + gamma * max Q(sâ€™,aâ€™)
#         #Note that our target is a matrix, with possible fields not only for the action taken but also
#         #for the other possible actions. The actions not take the same value as the prediction to not affect them
#         targets = np.zeros((inputs.shape[0], num_actions))
        
#         #We draw states to learn from randomly
#         for i, idx in enumerate(np.random.randint(0, len_memory,
#                                                   size=inputs.shape[0])):
#             """
#             Here we load one transition <s, a, r, sâ€™> from memory
#             state_t: initial state s
#             action_t: action taken a
#             reward_t: reward earned r
#             state_tp1: the state that followed sâ€™
#             """
#             state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]
            
#             #We also need to know whether the game ended at this state
#             game_over = self.memory[idx][1]

#             #add the state s to the input
#             inputs[i:i+1] = state_t
            
#             # First we fill the target values with the predictions of the model.
#             # They will not be affected by training (since the training loss for them is 0)
#             targets[i] = model.predict(state_t)[0]
            
#             """
#             If the game ended, the expected reward Q(s,a) should be the final reward r.
#             Otherwise the target value is r + gamma * max Q(sâ€™,aâ€™)
#             """
#             #  Here Q_sa is max_a'Q(s', a')
#             Q_sa = np.max(model.predict(state_tp1)[0])
            
#             #if the game ended, the reward is the final reward
#             if game_over:  # if game_over is True
#                 targets[i, action_t] = reward_t
#             else:
#                 # r + gamma * max Q(sâ€™,aâ€™)
#                 targets[i, action_t] = reward_t + self.discount * Q_sa
#         return inputs, targets

# # Define the model
# model = baseline_model(grid_size,num_actions,hidden_size)
# model.summary()
# # Define environment/game
# env = Catch(grid_size)
# # Initialize experience replay object
# exp_replay = ExperienceReplay(max_memory=max_memory)

# test(model)

# plt.plot(moving_average_diff(hist))
# plt.ylabel('Average of victories per game')
# plt.show()



# ++++++++++++ KERAS EG ++++++++++++

# import numpy as np 
# from keras.models import Sequential
# from keras.layers import Dense, Dropout

# x_train = np.random.random((100, 20))
# y_train = np.random.randint(2, size=(100, 1))
# x_test = np.random.random((100,20))
# y_test = np.random.randint(2, size=(100,1))

# model = Sequential()
# model.add(Dense(64, input_dim=20, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(64, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(1, activation='sigmoid'))

# model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
# model.fit(x_train, y_train, epochs=20, batch_size=128)
# score = model.evaluate(x_test, y_test, batch_size=128)
